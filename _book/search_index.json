[
["index.html", "Analítica Urbana ¿Para quién es esto? Antes de empezar", " Analítica Urbana Antonio Vazquez Brust &amp; Angie Scetta 2020-06-19 ¿Para quién es esto? Este libro fue escrito pensando en aquellas personas que trabajan, investigan y enseñan en áreas relacionadas al hábitat urbano y sus políticas públicas. Antes de empezar Se requiere conocimiento básico del lenguaje de programación R, y del “paquete” de funciones para manipulación y visualización de datos llamado Tidyverse. Todo ello puede adquirirse pasando un tiempo con Ciencia de Datos para Gente Sociable, que además de gratuito y disponible en línea, es el manual que sirve como base para éste que están leyendo ahora. Ciencia de Datos para Gente Sociable vendría a ser la primera parte, que enseña los primeros pasos, y ahora entramos en técnicas especializadas. Para practicar los ejemplos que se explicarán a lo largo del libro es necesario instalar el lenguaje de programación R, y la interfaz gráfica RStudio Desktop. "],
["intro.html", "Capítulo 1 Introducción 1.1 Sobre la analítica urbana", " Capítulo 1 Introducción En el siglo XXI la humanidad se ha vuelto, de forma quizás definitiva, una especie urbana. La mayoría de las personas del planeta viven en ciudades, y muchísimas se han trasladado en los últimos años, en un proceso global de migración desde regiones rurales que aún continúa. Nuestras ciudades crecen muy rápido, superando nuestra capacidad de planificación, mientras el incremento de demanda genera una enorme presión sobre los servicios que las ciudades brindan. Es evidente que vamos a necesitar redoblar esfuerzos para llevar a cabo una gestión apropiada que permita sostener los servicios que las ciudades brindan, pero no sólo ello: también mejorar la calidad de vida de las personas que allí habitan, a la vez que preservamos el ambiente. ¿Cómo podemos prepararnos para semejante desafío desde nuestro lugar de urbanistas? Es aquí donde entran en juego los cambios tecnológicos ocurridos en los últimos años. Estos cambios presentan un gran desafío tanto en términos de gestión cómo de planificación urbana. Hasta ahora nunca habíamos podido acceder a tantos datos, ni a tanta capacidad para procesarlos, comprenderlos y utilizarlos para tomar mejores decisiones. Hoy en día todas las personas generamos datos en forma constante al interactuar con nuevas tecnologías. Cuando utilizamos el transporte público con algún sistema de boleto electrónico, cuando participamos en las redes sociales, o cuando utilizamos algún dispositivo digital, estamos generando una masiva cantidad de datos, que superan con creces el volumen, la variedad y la velocidad que existe en los clásicos registros administrativos que los gobiernos utilizan hace años. Entonces, entendiendo que las ciudades son los sitios donde más personas habitan, es lógico que allí se genere la mayor cantidad de información. Contar con estos grandes volúmenes de datos puede, y debe, servir para que los gobiernos tomen decisiones basadas en evidencia. Es decir, que puedan caracterizar, cuantificar, y comparar tanto los esfuerzos vertidos como los resultados logrados en los proyectos de gestión pública al contar con resultados y alternativas comparables para elegir la calidad de vida de los ciudadanos. Pero ¿Cómo podemos hacer para aprovechar todos estos datos que se generan de forma masiva y constante? La respuesta es simple: Para comprender el presente y planificar el futuro aprovechando los datos de las ciudades debemos hacer uso de herramientas de analítica urbana, las cuales serán desarrolladas a lo largo de este libro. 1.1 Sobre la analítica urbana 1.1.0.1 ¿Qué es? La analítica urbana es un campo multidisciplinario que abarca conocimientos de tipo teórico sobre las cuestión urbana, ayudando a pensar en el “qué”, en cuales son las preguntas de interés, junto a capacidades técnicas que habilitan el “cómo”: permiten extraer conocimiento de los datos para describir algo que ya sucedió (estadística descriptiva) y realizar predicciones de algo que ocurrirá (modelos predictivos). 1.1.0.2 ¿Para qué se usa? Las herramientas de analítica urbana se utilizan para entender a las ciudades y sus habitantes a partir de diferentes datos relacionados a los campos del hábitat, desarrollo urbano, medios de transporte, tráfico, salud, educación, o seguridad, entre otros. Las herramientas de análisis de datos en gran escala permiten estudiar dinámicas urbanas en su infinidad de vertientes, y extraer conclusiones que ayuden a diseñar y planificar políticas públicas apropiadas para los objetivos que se persigan. A lo largo de este libro vamos a aprender algunos ejemplos de casos de aplicación de herramientas de análisis especializadas. Entre ellas: acceso a información urbana georreferenciada desde repositorios online, herramientas de geo-procesamiento de datos, análisis de dinámicas espacio-temporales, análisis de flujos urbanos, y por último, predicción de datos a partir de técnicas de aprendizaje automático o Machine Learning. "],
["geoprocesamiento.html", "Capítulo 2 Geoprocesamiento 2.1 Cruces espaciales 2.2 Uniones 2.3 Intersecciones 2.4 Distancias 2.5 Cálculo de extensión (longitud, área) 2.6 Ejercicios", " Capítulo 2 Geoprocesamiento Cuando tenemos información geográfica y queremos manipularla para generar nuevos datos que nos permitan hacer análisis espaciales, debemos utilizar diferentes herramientas de geoprocesamiento. Los geoprocesos son operaciones que se llevan a cabo con capas geográficas, que para nosotros toman la forma de dataframes espaciales. Algunas de estas operaciones se aplican a una sola capa geográfica (como agregar un área de influencia alrededor de un polígono), y otras a varias (como calcular la interseción entre una línea y un polígono, o estimar la distancia entre dos puntos). Existen operaciones para unir, recortar, disolver, borrar, fusionar, interseccionar, y calcular áreas de influencia (llamadas buffers), entre otras. En este capítulo aprenderemos a usar varias de ellas, incluidas en el paquete sf. 2.1 Cruces espaciales Hay ocasiones en que necesitamos cruzar datos de fuentes distintas en base a su ubicación geográfica. Es decir, un “join” que cruce registros en base a sus coordenadas espaciales, en lugar de otros atributos. Aquí va un ejemplo como guía para realizar el spatial join, o join espacial, que sólo puede ser realizado entre dataframes de tipo espacial. Paquetes que vamos a usar: library(tidyverse) library(sf) 2.1.1 Dataframes tradicionales y dataframes espaciales Vamos a trabajar con dos datasets. Uno contiene los alojamientos ofrecidos por Airbnb en Buenos Aires en Julio 2017. airbnb &lt;- read.csv(&quot;https://query.data.world/s/55amvafrknrgkeyeiu54yb2c6u6brc&quot;, stringsAsFactors = FALSE, encoding = &quot;UTF-8&quot;) names(airbnb) ## [1] &quot;room_id&quot; &quot;host_id&quot; &quot;room_type&quot; ## [4] &quot;country&quot; &quot;city&quot; &quot;neighborhood&quot; ## [7] &quot;address&quot; &quot;reviews&quot; &quot;overall_satisfaction&quot; ## [10] &quot;accommodates&quot; &quot;bedrooms&quot; &quot;bathrooms&quot; ## [13] &quot;price&quot; &quot;deleted&quot; &quot;minstay&quot; ## [16] &quot;last_modified&quot; &quot;latitude&quot; &quot;longitude&quot; ## [19] &quot;survey_id&quot; &quot;location&quot; &quot;coworker_hosted&quot; ## [22] &quot;extra_host_languages&quot; &quot;name&quot; &quot;property_type&quot; ## [25] &quot;currency&quot; &quot;rate_type&quot; Y el otro contiene los polígonos de las comunas porteñas: comunas &lt;- st_read(&#39;https://bitsandbricks.github.io/data/CABA_comunas.geojson&#39;) ## Reading layer `CABA_comunas&#39; from data source `https://bitsandbricks.github.io/data/CABA_comunas.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 15 features and 4 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -58.53152 ymin: -34.70529 xmax: -58.33514 ymax: -34.52754 ## CRS: 4326 Notemos que tenemos dos tipos de dataframe distintos. El de Airbnb es un dataframe “tradicional”, dado que todas sus columnas contiene valores simples: un número, un texto, un factor, etc. El dataframe de comunas es especial porque es “espacial”. Contiene una columna distinta a las demás, llamada “geometry” que en lugar de una observación simple contiene una lista con múltiples posiciones. Estas posiciones son los vértices que definen el polígono de cada comuna, y permiten la proyección en mapas y el cálculo de estadísticas espaciales. 2.1.2 Combinando datasets con información espacial Si lo único que queremos es visualizar en forma combinada la información que contienen, no hay problema en que un dataframe sea espacial y otro no, siempre y cuando éste último incluya una columna con latitud y otra con longitud para identificar la posición de cada registro. Dado que los datos de Airbnb incluyen lat/long, es fácil visualizarlos en conjunto con el dataframe espacial de las comunas: ggplot() + geom_sf(data = comunas) + geom_point(data = airbnb, aes(x = longitude, y = latitude), alpha = .3, color = &quot;orange&quot;) Dicho esto, si lo que queremos es combinar la información para su análisis cuantitativo, no nos alcanza con la visualización. Lo que tenemos que hacer es un “join espacial”, la técnica que permite cruzar datasets en base a sus atributos de ubicación geográfica. Sólo es posible hacer joins espaciales entre dataframes espaciales. Es por eso que los datos de Airbnb, así como están, no sirven para un join. ¡Pero! una vez más, dado que incluyen columnas de latitud y longitud, la solución es fácil. Podemos usar las columnas de lat/long para convertirlo en un dataset espacial hecho y derecho, así: airbnb &lt;- airbnb %&gt;% filter(!is.na(latitude), !is.na(longitude)) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) Tres cosas importantes a tener en cuenta: Un dataframe espacial no permite filas sin posición (sin coordenadas). Por eso antes de la conversión usamos filter(!is.na(latitude), !is.na(longitude)) para descartar los registros sin coordenadas del dataset de origen si los hubiera. La función st_as_sf() es la que toma un dataframe común y lo transforma en uno espacial. Con el parámetro coords = c(“longitude”, “latitude”) le definimos como se llaman las columnas de longitud y latitud, en ese orden. Obsérvese que toma los nombres entre comillas. El último parámetro, “crs”, es obligatorio y requiere el identificador del sistema de referencia de las coordenadas. Cuando se trata de datos capturados en internet (como aquí, por scraping del sitio de Airbnb), el crs siempre es 4326. Ahora que ambos dataframes son de tipo espacial, ambos se grafican con geom_sf() ggplot() + geom_sf(data = comunas) + geom_sf(data = airbnb, color = &quot;orange&quot;, alpha = .3) y más importante aún, se pueden combinar con un join espacial. La versión más simple, que combina atributos de las filas cuyas posiciones coinciden en el espacio, es así: airbnb_con_comunas &lt;- st_join(airbnb, comunas) El resultado es un dataframe con datos de Airbnb, que en cada fila incluye los datos de la comuna con la que coincide el alojamiento: head(airbnb_con_comunas) ## Simple feature collection with 6 features and 28 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -58.41829 ymin: -34.62068 xmax: -58.37914 ymax: -34.59101 ## CRS: EPSG:4326 ## room_id host_id room_type country city neighborhood ## 1 15125458 95870458 Private room NA NA NA ## 2 1691316 3380366 Private room NA NA NA ## 3 16069975 104686791 Private room NA NA NA ## 4 4470484 2034113 Private room NA NA NA ## 5 3564816 17949594 Private room NA NA NA ## 6 4479962 8875440 Private room NA NA NA ## address reviews overall_satisfaction accommodates bedrooms ## 1 Recoleta, Buenos Aires 23 4.5 1 1 ## 2 Palermo, Buenos Aires 106 5.0 2 1 ## 3 Recoleta, Buenos Aires 5 5.0 1 1 ## 4 Buenos Aires 102 4.5 2 1 ## 5 San Nicolás, Buenos Aires 20 4.5 1 1 ## 6 Balvanera, Буэнос-Айрес 102 4.5 2 1 ## bathrooms price deleted minstay last_modified survey_id ## 1 NA 339 0 NA 2017-07-03T17:52:23Z 1 ## 2 NA 559 0 NA 2017-07-03T17:52:26Z 1 ## 3 NA 254 0 NA 2017-07-03T17:52:26Z 1 ## 4 NA 441 0 NA 2017-07-03T17:52:26Z 1 ## 5 NA 390 0 NA 2017-07-03T17:52:26Z 1 ## 6 NA 424 0 NA 2017-07-03T17:52:26Z 1 ## location coworker_hosted ## 1 0101000020E61000000FB743C362324DC0B1DEA815A64B41C0 NA ## 2 0101000020E6100000780E65A88A354DC08A027D224F4C41C0 NA ## 3 0101000020E610000002D4D4B2B5324DC0B5C2F4BD864C41C0 NA ## 4 0101000020E6100000187C9A9317314DC04293C492724F41C0 NA ## 5 0101000020E61000004FC939B187304DC03D29931ADA4C41C0 NA ## 6 0101000020E6100000A6272CF180324DC0A88FC01F7E4E41C0 NA ## extra_host_languages name property_type ## 1 {en} Habitacion privada en Recoleta Apartment ## 2 {en} Palermo Cozy room wprivate bathroom House ## 3 {en} Habitacion Recoleta, Confort Condominium ## 4 {} The Flan Room- Palacio Nr San Telmo House ## 5 {en} &quot;Antique&quot; room @ El Centro Apartment ## 6 {} Private room - Congreso House ## currency rate_type ## 1 ARS nightly ## 2 ARS nightly ## 3 ARS nightly ## 4 ARS nightly ## 5 ARS nightly ## 6 ARS nightly ## barrios ## 1 RECOLETA ## 2 PALERMO ## 3 RECOLETA ## 4 CONSTITUCION - MONSERRAT - PUERTO MADERO - RETIRO - SAN NICOLAS - SAN TELMO ## 5 CONSTITUCION - MONSERRAT - PUERTO MADERO - RETIRO - SAN NICOLAS - SAN TELMO ## 6 BALVANERA - SAN CRISTOBAL ## perimetro area comunas geometry ## 1 21246.61 6140873 2 POINT (-58.39364 -34.59101) ## 2 21768.07 15772496 14 POINT (-58.41829 -34.59616) ## 3 21246.61 6140873 2 POINT (-58.39617 -34.59786) ## 4 35572.65 17802807 1 POINT (-58.38353 -34.62068) ## 5 35572.65 17802807 1 POINT (-58.37914 -34.60041) ## 6 10486.26 6385991 3 POINT (-58.39456 -34.61322) Con los atributos adicionales, podemos realizar sumarios por comuna de los alojamientos: airbnb_con_comunas %&gt;% group_by(comunas) %&gt;% summarise(cantidad = n()) ## Simple feature collection with 16 features and 2 fields ## geometry type: GEOMETRY ## dimension: XY ## bbox: xmin: -58.7976 ymin: -34.82524 xmax: -58.18856 ymax: -34.41952 ## CRS: EPSG:4326 ## # A tibble: 16 x 3 ## comunas cantidad geometry ## &lt;fct&gt; &lt;int&gt; &lt;GEOMETRY [°]&gt; ## 1 1 2100 MULTIPOINT ((-58.39242 -34.60016), (-58.39235 -34.60115), (… ## 2 10 18 MULTIPOINT ((-58.52669 -34.62379), (-58.52227 -34.61793), (… ## 3 11 34 MULTIPOINT ((-58.52372 -34.60679), (-58.51784 -34.61295), (… ## 4 12 163 MULTIPOINT ((-58.51262 -34.58124), (-58.50698 -34.56917), (… ## 5 13 670 MULTIPOINT ((-58.47438 -34.53906), (-58.47433 -34.55242), (… ## 6 14 3281 MULTIPOINT ((-58.44776 -34.56799), (-58.44745 -34.56847), (… ## 7 15 500 MULTIPOINT ((-58.49529 -34.593), (-58.48774 -34.58506), (-5… ## 8 2 1659 MULTIPOINT ((-58.41587 -34.59755), (-58.41514 -34.5967), (-… ## 9 3 468 MULTIPOINT ((-58.41385 -34.61058), (-58.41363 -34.60651), (… ## 10 4 136 MULTIPOINT ((-58.4205 -34.64693), (-58.42022 -34.64792), (-… ## 11 5 422 MULTIPOINT ((-58.4313 -34.6024), (-58.43129 -34.60154), (-5… ## 12 6 184 MULTIPOINT ((-58.45786 -34.60974), (-58.45726 -34.61023), (… ## 13 7 62 MULTIPOINT ((-58.47123 -34.62169), (-58.47018 -34.62031), (… ## 14 8 1 POINT (-58.46999 -34.66616) ## 15 9 15 MULTIPOINT ((-58.5287 -34.64678), (-58.52213 -34.63636), (-… ## 16 &lt;NA&gt; 210 MULTIPOINT ((-58.7976 -34.66354), (-58.69091 -34.47598), (-… El resultado de un join espacial también es un dataframe espacial, así que podemos visualizarlo de la manera habitual (y ahora tenemos más variables para graficar). ggplot() + geom_sf(data = comunas) + geom_sf(data = airbnb_con_comunas, aes(color = comunas)) 2.1.3 Coropletas Hasta acá obtuvimos un dataframe espacial con la geometría de puntos proveniente de los datos de Airbnb y generamos una nueva columna que indica a que Comuna pertenece cada uno de los registros. Sin embargo, cuando tenemos muchos puntos concentrados en un mapa resulta difícil realizar un análisis visual que nos permita sacar conclusiones de los datos. Para esto nos sirven las coropletas o mapas coropléticos, que muestran áreas geográficas (polígonos) coloreadas según alguna de las variables incluidas en el dataset. Continuando con el análisis de Airbnb, haremos un mapa coroplético dándole color a cada Comuna según la cantidad de propiedades que hay en alquiler. Veamos esto en detalle: Lo primero que debemos hacer es filtrar solo las propiedades que se ubican dentro de CABA y agruparlas por Comuna calculando la cantidad de observaciones que contiene cada una: comunas_airbnb &lt;- airbnb_con_comunas %&gt;% filter(!is.na(comunas)) %&gt;% group_by(comunas) %&gt;% summarise(cantidad=n()) Ahora quitemos la geometría de los puntos (columna geometry), que ya no la necesitaremos, y dejemos nuestro dataframe espacial como un dataframe tradicional: comunas_airbnb &lt;- comunas_airbnb %&gt;% st_set_geometry(NULL) head(comunas_airbnb) ## # A tibble: 6 x 2 ## comunas cantidad ## &lt;fct&gt; &lt;int&gt; ## 1 1 2100 ## 2 10 18 ## 3 11 34 ## 4 12 163 ## 5 13 670 ## 6 14 3281 Listo, juntemos la información que nos interesa de ambos dataframes a partir de un left_join() entre el shape original con los polígonos de las Comunas y los datos que agrupamos en el paso anterior: comunas &lt;- comunas %&gt;% left_join(comunas_airbnb, by=&quot;comunas&quot;) head(comunas) ## Simple feature collection with 6 features and 5 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -58.4627 ymin: -34.6625 xmax: -58.33514 ymax: -34.56935 ## CRS: 4326 ## barrios ## 1 CONSTITUCION - MONSERRAT - PUERTO MADERO - RETIRO - SAN NICOLAS - SAN TELMO ## 2 RECOLETA ## 3 BALVANERA - SAN CRISTOBAL ## 4 BARRACAS - BOCA - NUEVA POMPEYA - PARQUE PATRICIOS ## 5 ALMAGRO - BOEDO ## 6 CABALLITO ## perimetro area comunas cantidad geometry ## 1 35572.65 17802807 1 2100 MULTIPOLYGON (((-58.36854 -... ## 2 21246.61 6140873 2 1659 MULTIPOLYGON (((-58.39521 -... ## 3 10486.26 6385991 3 468 MULTIPOLYGON (((-58.41192 -... ## 4 36277.44 21701236 4 136 MULTIPOLYGON (((-58.3552 -3... ## 5 12323.47 6660526 5 422 MULTIPOLYGON (((-58.41287 -... ## 6 10990.96 6851029 6 184 MULTIPOLYGON (((-58.43061 -... Ya estamos en condiciones de hacer nuestro primer mapa coroplético a partir de la unión de un dataframe espacial y un dataframe tradicional. Para reconocer fácilmente las comunas, agreguemos etiquetas con geom_sf_text(): ggplot() + geom_sf(data = comunas, aes(fill=cantidad)) + geom_sf_text(data=comunas, aes(label = comunas), size=2.5, colour = &quot;black&quot;) + labs(title = &quot;Oferta de Airbnb por Comuna&quot;, subtitle = &quot;Propiedades publicadas&quot;, fill = &quot;Cantidad&quot;, caption= &quot;Fuente: Airbnb 2017&quot;, y=&quot;&quot;, x=&quot;&quot;) + scale_fill_gradient(low=&quot;khaki2&quot;, high=&quot;deeppink4&quot;) Tal como se ve en el mapa, a lo largo del corredor Norte de la Ciudad, y más específicamente en la Comuna 14, es donde Airbnb tiene la mayor cantidad de propiedades publicadas. Esto tiene sentido ya que Airbnb es un servicio orientado a turistas, y estas Comunas son las que tienen la mayor cantidad de atracciones turísticas. Sin embargo, para que estos datos sean comparables entre las 15 Comunas, relacionemos la cantidad de observaciones que tienen con la superficie (ha) de cada una: ggplot() + geom_sf(data = comunas, aes(fill=(cantidad/area)*10000)) + geom_sf_text(data=comunas, aes(label = comunas), size=2.5, colour = &quot;black&quot;)+ labs(title = &quot;Oferta de Airbnb por Comuna&quot;, subtitle = &quot;Densidad de propiedades publicadas&quot;, fill = &quot;Cantidad por ha&quot;, caption= &quot;Fuente: Airbnb 2017&quot;, y=&quot;&quot;, x=&quot;&quot;) + scale_fill_gradient(low=&quot;khaki2&quot;, high=&quot;deeppink4&quot;) Aquí pudimos ver que los resultados son similares a los del primer mapa coroplético, y que el corredor Norte sigue siendo la zona donde se concentra la mayor parte de la oferta. Pero si tenemos en cuenta la superficie total de las Comunas, notamos que la 2 (Recoleta) es la que más densidad de observaciones tiene, seguida por la Comuna 14. 2.2 Uniones Cuando realizamos análisis espaciales, es muy común que nos encontremos con la necesidad de combinar información geográfica. Esto se resuelve fácilmente a partir de un geoproceso llamado unión, que en el paquete sf lo encontraremos como st_union. Esta herramienta tiene 2 funcionalidades: Unir registros de una misma capa, generando un único dato. Unir las geometrías de dos capas, generando una capa única que contenga la información de ambas. Empecemos combinando los registros de nuestro dataframe espacial de comunas de manera tal que generemos un nuevo dataframe que contenga un único polígono de la Ciudad: caba &lt;- comunas %&gt;% st_union() %&gt;% st_as_sf(crs = 4326) Veamos cuantos registros tiene el dataframe de comunas: dim(comunas) ## [1] 15 6 Revisemos el resultado de nuestra unión: dim(caba) ## [1] 1 1 Efectivamente, pasamos de tener registros para las 15 comunas, a tener tan solo 1. Pero veamos esto en un mapa para entender mejor como funciona la unión: ggplot()+ geom_sf(data=caba) Tal como lo imaginamos, en el mapa vemos que se unieron las 15 Comunas y ya no aparecen sus límites. Quedó un solo polígono con la forma de toda la Ciudad. Pero esto no es todo. Como se mencionó anteriormente, también se pueden combinar/unir capas diferentes, generando un único dataframe espacial. Por ejemplo, seleccionemos las 2 comunas con mayor densidad de publicaciones de Airbnb y generamos un dataframe para cada una: comuna_2 &lt;- comunas %&gt;% filter(comunas==2) comuna_14 &lt;- comunas %&gt;% filter(comunas==14) Veamos como lucen: ggplot()+ geom_sf(data=comuna_2)+ geom_sf_text(data=comuna_2, aes(label = comunas), size=4, colour = &quot;black&quot;)+ geom_sf(data=comuna_14)+ geom_sf_text(data=comuna_14, aes(label = comunas), size=4, colour = &quot;black&quot;) Y ahora unamos: comuna_14_2 &lt;- st_union(comuna_14, comuna_2) ggplot()+ geom_sf(data=comuna_14_2, fill=&quot;orange&quot;) Como se puede ver en el mapa, al aplicar st_union se combinaron las 2 comunas que estaban en dataframes separados y se generó un único dataframe espacial con toda la información. 2.3 Intersecciones Muchas veces nos encontramos con que 2 capas geográficas se solapan y queremos obtener como resultado los datos que se intersectan. Para este tipo de análisis, utilizamos st_intersection de nuestro ya conocido paquete sf. Hacer una intersección entre 2 capas significa que ambas geometrías serán recortadas, generando una nueva capa que contenga solo las entidades que se encuentran superpuestas. Llevemos esto a datos reales y veamos un ejemplo de como crear un dataframe que contenga solo las observaciones de Airbnb que intersecten con el polígono de CABA creado en el paso anterior, y así eliminar todos los registros ubicados en AMBA. dim(airbnb_con_comunas) ## [1] 9923 29 Antes de aplicar la función, nos encontramos con que hay un total de 9.923 observaciones. Veamos que sucede después: airbnb_con_comunas &lt;- st_intersection(airbnb_con_comunas, caba) dim(airbnb_con_comunas) ## [1] 9713 29 Según los resultados obtenidos, son 9.713 las observaciones que pertenecen a la Ciudad. Veamos esto en un mapa: ggplot()+ geom_sf(data=caba) + geom_sf(data=airbnb_con_comunas, size=0.75, alpha = .3, color=&quot;orange&quot;) Listo, todo funcionó bien asi que ya tenemos nuestro dataframe espacial de Airbnb recortado a partir de una intersección espacial con CABA. Ahora hagamos una prueba más e intersectemos solo con las observaciones localizadas en las Comunas 2 y 14 filtradas anteriormente: airbnb_comuna_14_2 &lt;- airbnb_con_comunas %&gt;% st_intersection(comuna_14_2) ggplot() + geom_sf(data=comunas) + geom_sf(data=airbnb_comuna_14_2, aes(color=comunas), size=0.4, alpha=0.3) dim(airbnb_comuna_14_2) ## [1] 4940 39 De las 9.713 observaciones que vimos que caen en CABA, 4.940 caen en las comunas analizadas. Tal como era de esperar, esto representa más de un 50% de la muestra. 2.4 Distancias Ahora veamos como calcular distancias euclideanas entre datos espaciales, lo cual resulta muy útil a la hora de comprender como se organizan y relacionan entre sí diferentes entidades geográficas. Es decir que, esta herramienta nos permitirá agregar información a nuestros datos a partir de la relación espacial que tienen con otros datos. En sf, esto lo encontraremos bajo el nombre de st_distance. Comencemos con algo simple: Calculemos la distancia que hay entre los centroides de las 2 comunas con mayor densidad de oferta de Airbnb. Para establecer un único punto en cada comuna y poder realizar el cálculo, vamos a utilizar sus centroides que los calcularemos con la función st_centroid: comuna_2 &lt;- comuna_2 %&gt;% st_centroid() comuna_14 &lt;- comuna_14 %&gt;% st_centroid() Veamos el resultado en un mapa: ggplot()+ geom_sf(data=comunas, color=&quot;gray&quot;)+ geom_sf(data=comuna_2, color=&quot;red&quot;, shape=4, stroke=2, size=1)+ geom_sf(data=comuna_14, color=&quot;blue&quot;, shape=4, stroke=2, size=1) Y ahora calculemos la distancia entre ambos: st_distance(comuna_2, comuna_14) ## Units: [m] ## [,1] ## [1,] 2847.48 Nos encontramos con que la distancia lineal entre el centroide de la Comuna 2 y el de la Comuna 14 es de 2.847,48 metros, es decir, unas 28 cuadras aproximadamente. Veamos un ejemplo más: Veamos a cuántos metros del centroide de la Comuna 2 se encuentran el resto de propiedades publicadas. Calcularemos el resultado en una nueva columna de nuestro dataframe llamada dist_comuna2: airbnb_con_comunas &lt;- airbnb_con_comunas %&gt;% mutate(dist_comuna2 = st_distance(airbnb_con_comunas, comuna_2)) Analicemos los resultados: summary(airbnb_con_comunas$dist_comuna2) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 21.92 1786.36 2928.83 3202.80 4149.21 13963.07 Vemos que las propiedades más cercanas, se ubican a 21,92 metros, y las más lejanas a 13.963,07 metros. En promedio, todos los registros están ubicados a 3.202,8 metros del centroide de la Comuna 2, es decir, aproximadamente 32 cuadras. Revisemos la distribución en un histograma: ggplot(airbnb_con_comunas)+ geom_histogram(aes(x=as.numeric(dist_comuna2))) Y en un mapa: ggplot()+ geom_sf(data=caba, color=&quot;gray&quot;)+ geom_sf(data=airbnb_con_comunas, aes(color=as.numeric(dist_comuna2)))+ geom_sf(data=comuna_2, fill=NA, shape=4, stroke=2, size=1.5)+ scale_color_viridis_c(direction = -1)+ labs(title=&quot;Distancia a la Comuna 2&quot;, subtitle=&quot;Oferta Airbnb 2017&quot;, color=&quot;Distancia&quot;, x=&quot;&quot;, y=&quot;&quot;) Tanto en el histograma como en el mapa se aprecia perfectamente como la mayoría de las propiedaddes se ubican a menos de 5.000 metros del centroide de la Comuna. Bonus Track Hagamos un poco más complejo el análisis aprovechando que los datos que estamos analizando en este capítulo pertenecen a la oferta de alquileres turísticos, y calculemos la distancia entre cada una de las propiedades publicadas en Airbnb y cada uno de los hoteles de la Ciudad, para entender si hay similitudes en como se distribuyen ambas ofertas en el territorio. Esto será posible a partir del armado de una función que nos permita calcular y filtrar la distancia de cada una de las propiedades publicadas en Airbnb al hotel más cercano. Primero cargamos el dataset de hoteles que está publicado en le portal de datos abiertos de GCBA: hoteles &lt;- read.csv(&quot;http://cdn.buenosaires.gob.ar/datosabiertos/datasets/alojamientos-turisticos/alojamientos-turisticos.csv&quot;, encoding = &quot;UTF-8&quot;) Y lo convertimos a shape como ya aprendimos: hoteles &lt;- hoteles %&gt;% filter(!is.na(lat), !is.na(long)) %&gt;% st_as_sf(coords = c(&quot;long&quot;, &quot;lat&quot;), crs = 4326) Luego creamos una nueva columna en donde calculamos la distancia de cada propiedad a cada hotel, pero nos quedamos únicamente con la distancia a la estación más cercana. airbnb_con_comunas &lt;- airbnb_con_comunas %&gt;% mutate(dist_hotel = apply(st_distance(airbnb_con_comunas, hoteles), 1, function(x) min(x))) Revisemos como se ve el resultado obtenido: airbnb_con_comunas %&gt;% select(room_type, name, dist_hotel) %&gt;% head() ## Simple feature collection with 6 features and 3 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -58.41829 ymin: -34.62068 xmax: -58.37914 ymax: -34.59101 ## CRS: EPSG:4326 ## room_type name dist_hotel ## 1 Private room Habitacion privada en Recoleta 142.02029 ## 2 Private room Palermo Cozy room wprivate bathroom 162.42654 ## 3 Private room Habitacion Recoleta, Confort 116.76407 ## 4 Private room The Flan Room- Palacio Nr San Telmo 38.54021 ## 5 Private room &quot;Antique&quot; room @ El Centro 90.17597 ## 6 Private room Private room - Congreso 203.75457 ## geometry ## 1 POINT (-58.39364 -34.59101) ## 2 POINT (-58.41829 -34.59616) ## 3 POINT (-58.39617 -34.59786) ## 4 POINT (-58.38353 -34.62068) ## 5 POINT (-58.37914 -34.60041) ## 6 POINT (-58.39456 -34.61322) summary(airbnb_con_comunas$dist_hotel) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.408 91.583 169.227 260.199 318.691 4828.336 La mínima distancia entre una propiedad publicada en Airbnb y un hotel es de 1,4 metros y la máxima es de 4.828 metros. Sin embargo, si miramos la media podemos ver que las propiedades se encuentran, en promedio, a 260 metros del hotel más cercano, lo cuál es muy cerca. A priori con estos datos, parecería ser que la localización de los Airbnb está muy relacionada con la de los hoteles. Visualicemos esto en un histograma para comprender mejor la distribución de los datos: ggplot(airbnb_con_comunas) + geom_histogram(aes(x = dist_hotel)) La mayor parte de las observaciones se encuentra a menos de 250 metros de algun hotel. Veamos esto en un mapa: airbnb_con_comunas %&gt;% ggplot() + geom_sf(data=comunas)+ geom_sf(aes(color=dist_hotel)) + geom_sf(data=hoteles) + scale_color_viridis_c(option = &quot;plasma&quot;, direction = -1) En el mapa se ve como la distribución de la oferta de propiedades no es aleatoria, sino que responde a la demanda de los turistas ya que la mayor parte está ubicada cerca de hoteles (puntos negros). Los patrones de los 2 dataset son muy similares: gran concentración de puntos en Recoleta, Retiro y Microcento, extendiéndose hacia el corredor norte y siendo casi nula hacia el sur de la Ciudad. 2.5 Cálculo de extensión (longitud, área) Otras herramientas muy importantes a la hora de realizar un análisis espacial son los cálculos de extensión de geometrías, esto hace referencia a calcular áreas o longitudes. En la analítica urbana, el cálculo de áreas suele utilizarse para medir superficies de comunas, barrios, radios censales o cualquier otro polígono que se ubique en el territorio. Y el cálculo de longitud suele realizarse para medir el perímetro de alguno de estos polígonos o la extensión de la red de subte, FFCC, etc. Para ambas herramientas, tenemos una función del paquete sf: estas son st_area y st_length. Veamos de que se tratan estas funciones y comencemos calculando la superficie y el perímetro de todo el polígono de la Ciudad de Buenos Aires. Para esto agreguemos 2 columnas al dataframe de CABA con el que veníamos trabajando previamente: caba &lt;- caba %&gt;% mutate(superficie=st_area(caba), perimetro=st_length(caba)) caba$superficie ## 203678002 [m^2] caba$perimetro ## 128613.8 [m] Los resultados de área y perímetro se expresan en m2 y m, pasemoslos a ha y km respectivamente: caba &lt;- caba %&gt;% mutate(superficie_ha=round(as.numeric(superficie)/10000, 2), perimetro_km=round(as.numeric(perimetro)/1000, 2)) caba$superficie_ha ## [1] 20367.8 caba$perimetro_km ## [1] 128.61 Listo! Con pocas líneas código ya pudimos conocer ambos resultados. Según nuestros cálculos, la superficie de toda la Ciudad de Buenos Aires es de 20.367,8 ha y el perímetro de 128,61 km. Veamos algunos ejemplos más: Calculemos la superficie de cada uno de los 48 barrios de la Ciudad y veamos cuales son los de mayor tamaño. Para esto comencemos cargando el geoJSON de barrios: barrios &lt;- st_read(&quot;https://raw.githubusercontent.com/angiescetta/datos-geo/master/barrios.geojson&quot;) ## Reading layer `barrios&#39; from data source `https://raw.githubusercontent.com/angiescetta/datos-geo/master/barrios.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 48 features and 2 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -58.53152 ymin: -34.70529 xmax: -58.33515 ymax: -34.52649 ## CRS: 4326 Creemos una nueva columna para calcular las superficies: barrios &lt;- barrios %&gt;% mutate(superficie=st_area(barrios)) head(barrios) ## Simple feature collection with 6 features and 3 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -58.50617 ymin: -34.63064 xmax: -58.41192 ymax: -34.57829 ## CRS: 4326 ## barrio comuna geometry superficie ## 1 CHACARITA 15 POLYGON ((-58.45282 -34.595... 3115708 [m^2] ## 2 PATERNAL 15 POLYGON ((-58.46558 -34.596... 2229830 [m^2] ## 3 VILLA CRESPO 15 POLYGON ((-58.42375 -34.597... 3615979 [m^2] ## 4 VILLA DEL PARQUE 11 POLYGON ((-58.49461 -34.614... 3399597 [m^2] ## 5 ALMAGRO 5 POLYGON ((-58.41287 -34.614... 4050753 [m^2] ## 6 CABALLITO 6 POLYGON ((-58.43061 -34.607... 6851031 [m^2] Nuevamente los resultados están en m2, hagamos una conversión a ha para facilitar la lectura de los mismos: barrios &lt;- barrios %&gt;% mutate(superficie_ha=round(as.numeric(superficie)/10000, 2)) select(barrios, barrio, superficie_ha) %&gt;% summary() ## barrio superficie_ha geometry ## AGRONOMIA: 1 Min. : 123.2 POLYGON :48 ## ALMAGRO : 1 1st Qu.: 222.2 epsg:4326 : 0 ## BALVANERA: 1 Median : 368.0 +proj=long...: 0 ## BARRACAS : 1 Mean : 424.3 ## BELGRANO : 1 3rd Qu.: 514.3 ## BOCA : 1 Max. :1584.6 ## (Other) :42 Ahora si, podemos ver que, las superficies de los barrios son bastante variadas: van desde 123 ha a 1.584 ha, siendo el promedio 424 ha. Entremos más en detalle y veamos cuántos son los barrios que tienen una superficie por encima de la media y cuántos por debajo. Para esto creemos una nueva columna llamada categoría: barrios &lt;- barrios %&gt;% mutate(categoria=ifelse(superficie_ha&gt;mean(superficie_ha),&quot;MAYOR SUPERFICIE&quot;,&quot;MENOR SUPERFICIE&quot;)) barrios %&gt;% group_by(categoria) %&gt;% summarise(cantidad=n()) ## Simple feature collection with 2 features and 2 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -58.53152 ymin: -34.70529 xmax: -58.33515 ymax: -34.52649 ## CRS: 4326 ## # A tibble: 2 x 3 ## categoria cantidad geometry ## &lt;chr&gt; &lt;int&gt; &lt;MULTIPOLYGON [°]&gt; ## 1 MAYOR SUPERF… 20 (((-58.43492 -34.64624, -58.43354 -34.64562, -58.43348… ## 2 MENOR SUPERF… 28 (((-58.44978 -34.68061, -58.44863 -34.68152, -58.44451… La mayoría de los barrios (28) tienen una superficie por debajo de la media, mientras que el resto (20) tienen una superficie por encima de la misma. Veamos esto en un mapa: ggplot()+ geom_sf(data=barrios, aes(fill=categoria))+ labs(title=&quot;Barrios de CABA según superficie (ha)&quot;) A simple vista, y en términos generales, se podría decir que, los barrios del Norte y Sur de la Ciudad son los que tienen la mayor superficie, mientras que los del Centro son los que tienen la menor. Veamos un último ejemplo: Calculemos la extensión de todas las líneas que componen la red de subterráneos de CABA para ver cuales son las que tienen mayor cobertura. Carguemos el geoJSON: subte_lineas &lt;- st_read(&quot;http://bitsandbricks.github.io/data/subte_lineas.geojson&quot;) ## Reading layer `subte_lineas&#39; from data source `http://bitsandbricks.github.io/data/subte_lineas.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 80 features and 2 fields ## geometry type: MULTILINESTRING ## dimension: XY ## bbox: xmin: -58.48639 ymin: -34.64331 xmax: -58.36993 ymax: -34.55564 ## CRS: 4326 Y calculemos longitud: subte_lineas &lt;- subte_lineas %&gt;% mutate(longitud=st_length(subte_lineas)) head(subte_lineas) ## Simple feature collection with 6 features and 3 fields ## geometry type: MULTILINESTRING ## dimension: XY ## bbox: xmin: -58.45649 ymin: -34.58516 xmax: -58.41596 ymax: -34.56231 ## CRS: 4326 ## ID LINEASUB geometry longitud ## 1 1 LINEA D MULTILINESTRING ((-58.45213... 803.7500 [m] ## 2 2 LINEA D MULTILINESTRING ((-58.45649... 599.8831 [m] ## 3 3 LINEA D MULTILINESTRING ((-58.44467... 1055.4827 [m] ## 4 4 LINEA D MULTILINESTRING ((-58.43501... 928.8651 [m] ## 5 5 LINEA D MULTILINESTRING ((-58.42571... 531.1796 [m] ## 6 6 LINEA D MULTILINESTRING ((-58.4212 ... 635.6720 [m] Aquí podemos ver que en el shape, cada línea de subte está construida por varias geometrías (líneas). Para calcular la longitud total de cada línea debemos sumar las longitudes de cada uno de los tramos que la componen: subte_lineas &lt;- subte_lineas %&gt;% group_by(LINEASUB) %&gt;% summarise(longitud=sum(as.numeric(longitud))) %&gt;% arrange(desc(longitud)) subte_lineas ## Simple feature collection with 6 features and 2 fields ## geometry type: MULTILINESTRING ## dimension: XY ## bbox: xmin: -58.48639 ymin: -34.64331 xmax: -58.36993 ymax: -34.55564 ## CRS: 4326 ## # A tibble: 6 x 3 ## LINEASUB longitud geometry ## &lt;fct&gt; &lt;dbl&gt; &lt;MULTILINESTRING [°]&gt; ## 1 LINEA B 11770. ((-58.39947 -34.60464, -58.40363 -34.60476, -58.4054 -34.60… ## 2 LINEA D 10462. ((-58.38057 -34.60425, -58.37945 -34.60484, -58.37532 -34.6… ## 3 LINEA A 9643. ((-58.46354 -34.62909, -58.46411 -34.62925, -58.46534 -34.6… ## 4 LINEA E 9641. ((-58.45789 -34.64014, -58.45835 -34.64048, -58.4595 -34.64… ## 5 LINEA H 7217. ((-58.40579 -34.63841, -58.40703 -34.63867, -58.40731 -34.6… ## 6 LINEA C 4437. ((-58.38143 -34.62762, -58.38146 -34.62704, -58.38146 -34.6… De las 6 líneas que aparecen en el dataframe, la que tiene mayor cobertura es la Línea B, seguida por la D. Y la que tiene menor cobertura es la C. También se puede ver que las líneas A y E tienen una longitud muy similar, con una diferencia de solo 2km. Llevemos esta información a un mapa: ggplot()+ geom_sf(data=caba)+ geom_sf(data=subte_lineas, aes(color=longitud), size=1)+ geom_sf_label(data=subte_lineas, aes(label = LINEASUB), size=1.5)+ scale_color_viridis_c(option = &quot;plasma&quot;, direction=-1)+ labs(title=&quot;Líneas de SUBTE según longitud (km)&quot;) 2.6 Ejercicios Adquiriendo open data urbana + Calculando y mapeando agregados por área I. Elegir una ciudad en cualquier parte del mundo que les interese y que disponga de un portal de datos abiertos que ofrece un shapefile con sus barrios. Del mismo portal de datos, o de otra fuente si la tienen, elegir un dataset con registros geo-referenciados. Por ejemplo, las escuelas de la ciudad (o las comisarías, o las propiedades en alquiler, o…) con sus coordenadas. Realizar un join espacial, asignando a cada registro geo-referenciado el barrio que le corresponde. Utilizando ggplot() realizar: Un gráfico (barras, puntos, o el que prefieran) para mostrar los resultados de cantidad por barrio. Un mapa con los límites de los barrios, cuyo color de relleno indique la cantidad encontrada en cada uno. "],
["acceso-a-información-urbana-georeferenciada-en-repositorios-online.html", "Capítulo 3 Acceso a información urbana georeferenciada en repositorios online 3.1 OpenStreetMap desde R 3.2 Un ejercicio más: ¡Bares en el barrio! 3.3 Ejercicios", " Capítulo 3 Acceso a información urbana georeferenciada en repositorios online OpenStreetMap es un servicio de mapas online que publica información contribuida en forma libre por más de un millón de voluntarios, que benefician a los 5,5 millones de usuarios de la plataforma. Los contribuidores más entusiastas mapean barrios completos utilizando herramientas GPS para enviar información local completa, actualizada y precisa a OpenStreetMap. Varias empresas y entidades públicas que producen información geográfica también contribuyen al permitir que sus datos sean incluidos. Existen equipos profesionales de contribuidores que que se coordinan para agregar y mantener actualizada información georeferenciada de límites políticos, calles, edificios, negocios y otros puntos de interés; en ocasiones empleados por compañías que dependen de OpenStreetMap para el “mapa base” de sus productos, como mapbox.com y carto.com. Toda la información disponible en OpenStreetMap puede ser descargada y reutilizada por cualquier persona, ya sea accediendo al mapa online, obteniendo una copia completa de la base de datos, o accediendo a los datos vía API. 3.1 OpenStreetMap desde R Utilizaremos osmdata, un paquete de R que permite acceder a los datos de OpenStreetMap (OSM de aquí en más) con sus atributos, geometría y posición. Como siempre, si no tenemos aún el paquete lo instalamos: install.packages(&quot;osmdata&quot;) También vamos a hacer uso del paquete leaflet, que nos va a permitir generar de forma muy rápida mapas interactivos. install.packages(&quot;leaflet&quot;) Y los activamos junto a otros paquetes que vamos a utilizar: library(osmdata) library(leaflet) library(tidyverse) # nuestra navaja suiza para manipulación y visualización de datos library(sf) # para procesar info espacial 3.1.1 Definiendo el lugar Antes de descargar información, definimos el lugar que queremos consultar. Éste puede ser un barrio, un municipio, un país, un continente… en este caso, lo intentaremos con la ciudad de Rosario. Las funciones de osmdata nos permiten realizar consultas a Overpass (http://overpass-api.de/), una interfaz que permite extraer información de la base de datos global de OpenStreetMap. Overpass requiere que se especifique una “bounding box”, es decir las coordenadas de un rectángulo que abarque la zona de interés. Podemos obtener la bounding box de cualquier lugar con la función getbb(): bbox &lt;- getbb(&quot;Rosario, Santa Fe&quot;) bbox ## min max ## x -60.78326 -60.61167 ## y -33.03487 -32.86965 Con getbb() también podemos obtener un polígono con los límites políticos, las fronteras exactas, de un lugar. Esto es muy útil para realizar mapas, o para filtrar la información que obtendremos luego para quedarnos sólo con la que corresponda a nuestra ciudad de interés, descartando la de áreas aledañas: bbox_poly &lt;- getbb(&quot;Municipio de Rosario, Santa Fe&quot;, format_out = &quot;sf_polygon&quot;) Para asegurarnos de que tenemos el lugar que queremos, y no otro de nombre similar en alguna otra parte del mundo, lo verificamos en un mapa rápido provisto vía leaflet: leaflet(bbox_poly) %&gt;% addTiles() %&gt;% addPolygons() Luce bien, así que continuamos. 3.1.2 Extrayendo información de OpenStreetMap El siguiente paso es utilizar la función add_osm_feature() para especificar los datos que queremos descargar. Esto requiere conocer las palabras clave con las que se identifican los registras en la base de OSM, que permiten indicar con gran detalle el tipo de datos georeferenciados que queremos: ya sean áreas de parques públicos, posición de oficinas de correo o cajeros automáticos, vías de ferrocarril… u otro, en un larguísimo etcétera que se puede consultar en https://wiki.openstreetmap.org/wiki/Map_Features En este caso vamos a solicitar todas las vías de circulación (calles, avenidas, autopistas, etc) de la ciudad. En la base de datos de OSM todas aparecen con la clave “highway”. rosario &lt;- opq(bbox) %&gt;% add_osm_feature(key = &quot;highway&quot;) Observemos que lo único que hemos obtenido hasta ahora es la definición de una consulta (qué y en dónde), pero aun no descargamos ningún dato: rosario ## $bbox ## [1] &quot;-33.0348662,-60.7832623,-32.8696532,-60.6116695&quot; ## ## $prefix ## [1] &quot;[out:xml][timeout:25];\\n(\\n&quot; ## ## $suffix ## [1] &quot;);\\n(._;&gt;;);\\nout body;&quot; ## ## $features ## [1] &quot; [\\&quot;highway\\&quot;]&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;list&quot; &quot;overpass_query&quot; Es sólo la definición de una consulta a la base de datos de OpenStreetMap: “Todas las calles (objetos con clave”highway“) dentro de éste rectángulo (que sabemos, corresponde a Rosario)”. Para hacer efectiva la consulta y descargar los datos, la pasamos por la función osmdata_sf() que recolecta lo que buscamos y lo entrega en forma de dataset espacial: rosario &lt;- rosario %&gt;% osmdata_sf() La descarga de información para una ciudad grande puede tomar varios minutos, y más aún la de un área metropolitana (o país, o continente, etc) así que es normal esperar un poco en ésta parte. En cuanto se completa, ya tenemos calles: rosario La consulta devolvió toda la información de puntos, líneas y polígonos disponibles en la base de OSM. A nos otros nos interesan las líneas, “osm_lines”, que demarcan la traza de las calles. Los registros con otras geometrías, como polígonos, pueden representar elementos asociados a las calles como bulevares o áreas de vereda que no vamos a usar por el momento. Del conjunto de datos disponibles, extraemos el dataframe con líneas, y chequeamos los atributos disponibles. Todos han sido recopilados por la comunidad de OpenStreetMap. calles &lt;- rosario$osm_lines # mostramos sólo las primeras 10 columnas... porque tiene montones! head(calles[1:10,]) ## Simple feature collection with 6 features and 285 fields ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: -60.71891 ymin: -33.01829 xmax: -60.65706 ymax: -32.86627 ## CRS: EPSG:4326 ## osm_id name abandoned.highway ## 10585611 10585611 Autopista Juan José Valle &lt;NA&gt; ## 23633084 23633084 Avenida de Circunvalación 25 de Mayo &lt;NA&gt; ## 23633086 23633086 Puente Nuestra Señora del Rosario &lt;NA&gt; ## 23633100 23633100 Avenida de Circunvalación 25 de Mayo &lt;NA&gt; ## 23633101 23633101 Avenida de Circunvalación 25 de Mayo &lt;NA&gt; ## 23644546 23644546 Avenida de Circunvalación 25 de Mayo &lt;NA&gt; ## abandoned.railway.left abandoned.surface abutters access access.lanes ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## addr.city addr.housenumber addr.postcode addr.street agricultural ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## alt_name amenity arcade.left arcade.right area ## 10585611 Ruta Nacional 9 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 Doctor Constantino Razzetti &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 Doctor Constantino Razzetti &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 Doctor Constantino Razzetti &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 Doctor Constantino Razzetti &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## barrier bench bicycle bicycle.lanes bicycle.oneway bicycle_road bin ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## bridge bridge.name bridge.structure bridge.support building bus ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 cantilever &lt;NA&gt; beam pylon &lt;NA&gt; &lt;NA&gt; ## 23633100 yes &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 viaduct &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## bus.lanes bus_bay busway.left busway.right change.backward ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## change.forward change.lanes change.lanes.backward change.lanes.forward ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## check_date construction construction.highway construction.lanes ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## construction.maxspeed construction.name construction.oneway ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## construction.surface contact.website conveying covered crossing ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## crossing.island crossing_ref cutting cycleway cycleway.left ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## cycleway.left.oneway cycleway.left.surface cycleway.right description ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## description.cycleway designation destination destination.backward ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; Victoria RN174 &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## destination.forward destination.lanes destination.ref ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; Victoria|Victoria|Rosario &lt;NA&gt; ## 23633101 &lt;NA&gt; Victoria|Victoria|Rosario &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## destination.ref.forward destination.ref.lanes destination.street ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; RN174|RN174|RN11 &lt;NA&gt; ## 23633101 &lt;NA&gt; RN174|RN174|RN11 &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## destination.street.lanes ## 10585611 &lt;NA&gt; ## 23633084 &lt;NA&gt; ## 23633086 &lt;NA&gt; ## 23633100 &lt;NA&gt; ## 23633101 Puente Rosario - Victoria|Puente Rosario - Victoria|Bulevar General José Rondeau ## 23644546 &lt;NA&gt; ## destination.symbol destination.symbol.forward destination.symbol.lanes ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; bridge|bridge|centre ## 23633101 &lt;NA&gt; &lt;NA&gt; bridge|bridge|centre ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## destination.to distance disused disused.highway disused.maxspeed ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## disused.turn.lanes.backward drinking_water embankment embedded_rails ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; yes &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## emergency emergency.lanes fence_type fixme flood_prone floor.material ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## foot foot.lanes footway golf golf_cart goods handrail handrail.center ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## handrail.left handrail.right hazard height hgv highway historic ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; motorway &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; motorway &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; motorway &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; motorway &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; motorway &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; motorway &lt;NA&gt; ## horse hov incline indoor indoor.highway int_name internet_access ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## internet_access.fee internet_access.operator junction kerb landcover ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## landuse lanes lanes.backward lanes.bicycle lanes.bus lanes.forward ## 10585611 &lt;NA&gt; 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## lanes.psv lanes.psv.conditional layer leisure level lit lit.type ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; yes &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; 1 &lt;NA&gt; &lt;NA&gt; yes &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; yes &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; 1 &lt;NA&gt; &lt;NA&gt; yes &lt;NA&gt; ## loc_ref man_made material maxheight maxheight.physical maxspeed ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 4.5 &lt;NA&gt; 120 ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 100 ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 80 ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 100 ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 100 ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 100 ## maxspeed.bus maxspeed.hgv maxspeed.lanes maxspeed.type memorial ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 90 80 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; 60 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 90 80 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 90 80 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 90 80 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## minspeed mooring motor_vehicle motor_vehicle.conditional ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 60 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 55 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 60 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 60 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 60 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## motor_vehicle.lanes motorcar motorcycle motorroad mtb.scale ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; yes &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; yes &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; yes &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; yes &lt;NA&gt; ## mtb.scale.uphill name.bridge name.etymology.wikidata natural network ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## noname note obstacle.wheelchair ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## official_name ## 10585611 Autopista Teniente General Juan José Valle ## 23633084 &lt;NA&gt; ## 23633086 &lt;NA&gt; ## 23633100 &lt;NA&gt; ## 23633101 &lt;NA&gt; ## 23644546 &lt;NA&gt; ## old_name old_ref oneway ## 10585611 Autopista Teniente General Pedro Aramburu &lt;NA&gt; yes ## 23633084 &lt;NA&gt; &lt;NA&gt; yes ## 23633086 &lt;NA&gt; &lt;NA&gt; yes ## 23633100 &lt;NA&gt; &lt;NA&gt; yes ## 23633101 &lt;NA&gt; &lt;NA&gt; yes ## 23644546 &lt;NA&gt; &lt;NA&gt; yes ## oneway.bicycle oneway.source opening_hours operator overtaking ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; forward ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; yes ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; no ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; forward ## park_ride parking parking.condition.both ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## parking.condition.both.maxstay parking.condition.both.residents ## 10585611 &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; ## parking.condition.both.time_interval parking.condition.left ## 10585611 &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; ## parking.condition.left.maxstay parking.condition.left.vehicles ## 10585611 &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; ## parking.condition.right parking.condition.right.maxstay ## 10585611 &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; ## parking.condition.right.time_interval parking.condition.right.vehicles ## 10585611 &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; ## parking.lane.both parking.lane.left parking.lane.left.both ## 10585611 &lt;NA&gt; no_stopping &lt;NA&gt; ## 23633084 &lt;NA&gt; no_stopping &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 no_stopping &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; no_stopping &lt;NA&gt; ## 23644546 &lt;NA&gt; no_stopping &lt;NA&gt; ## parking.lane.left.parallel parking.lane.right ## 10585611 &lt;NA&gt; no_parking ## 23633084 &lt;NA&gt; no_parking ## 23633086 &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; no_parking ## 23644546 &lt;NA&gt; no_parking ## parking.lane.right.diagonal parking.lanes.left parking.lanes.right ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## passenger_information_display picnic_table placement ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## placement.backward placement.forward proposed proposed.highway ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## proposed.lanes proposed.maxspeed proposed.name proposed.oneway ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## proposed.surface psv psv.lanes public_transport ramp ramp.bicycle ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## ramp.luggage ramp.stroller ramp.wheelchair ref reg_name ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; RN9 &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; RNA008 &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; RN174 &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; RNA008 &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; RNA008 &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; RNA008 &lt;NA&gt; ## seamark.bridge.category segregated service share_taxi shelter ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 suspension &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## short_name shoulder shoulder.access.bicycle shoulder.line ## 10585611 Juan José Valle &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 Avenida Circunvalación &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 Avenida Circunvalación &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 Avenida Circunvalación &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 Avenida Circunvalación &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## shoulder.right shoulder.right.surface shoulder.surface sidewalk ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## sidewalk.both.surface sidewalk.oneway sidewalk.right ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## sidewalk.right.surface smoothness sorting_name source source.bridge ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## source.cycleway source.hgv source.highway source.imagery source.lanes ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## source.maxheight source.maxspeed source.minspeed ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; sign sign ## 23633086 &lt;NA&gt; &lt;NA&gt; sign;Mapillary ## 23633100 &lt;NA&gt; sign sign ## 23633101 &lt;NA&gt; sign sign ## 23644546 &lt;NA&gt; sign sign ## source.motor_vehicle.conditional ## 10585611 &lt;NA&gt; ## 23633084 &lt;NA&gt; ## 23633086 &lt;NA&gt; ## 23633100 &lt;NA&gt; ## 23633101 &lt;NA&gt; ## 23644546 &lt;NA&gt; ## source.name ## 10585611 http://infoleg.mecon.gov.ar/infolegInternet/anexos/225000-229999/225220/norma.htm ## 23633084 &lt;NA&gt; ## 23633086 &lt;NA&gt; ## 23633100 &lt;NA&gt; ## 23633101 &lt;NA&gt; ## 23644546 &lt;NA&gt; ## source.oneway source.parking.lane.both source.parking.lanes.both ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## source.proposed source.proposed.lanes ## 10585611 &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; ## source.ref source.surface ## 10585611 http://forum.openstreetmap.org/viewtopic.php?id=31749 &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; ## sport step_count supervised surface surface.bicycle surface.lanes ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; asphalt &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; asphalt &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; asphalt &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; asphalt &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; asphalt &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; asphalt &lt;NA&gt; &lt;NA&gt; ## surface.lanes.backward surface.lanes.forward surveillance ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## surveillance.type tactile_paving taxi taxi.lanes toilets toll tourism ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; yes &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## tourist_bus tracktype traffic traffic_calming train trolley_wire ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## trolleybus tunnel turn turn.lanes ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; through|through|slight_right ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; through|through|slight_right ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## turn.lanes.backward turn.lanes.forward vehicle vehicle.conditional ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## water wheelchair wheelchair.description width wikidata ## 10585611 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633086 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633100 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23633101 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 23644546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## wikimedia_commons wikipedia geometry ## 10585611 &lt;NA&gt; &lt;NA&gt; LINESTRING (-60.66396 -33.0... ## 23633084 &lt;NA&gt; &lt;NA&gt; LINESTRING (-60.71845 -32.8... ## 23633086 &lt;NA&gt; &lt;NA&gt; LINESTRING (-60.68246 -32.8... ## 23633100 &lt;NA&gt; &lt;NA&gt; LINESTRING (-60.70536 -32.8... ## 23633101 &lt;NA&gt; &lt;NA&gt; LINESTRING (-60.7047 -32.87... ## 23644546 &lt;NA&gt; &lt;NA&gt; LINESTRING (-60.71891 -32.8... 3.1.3 Visualizando los resultados Dado que las calles han sido descargadas en formato sf, podemos visualizarlas con ggplot: y geom_sf: ggplot() + geom_sf(data = calles) Las calles exceden los límites de Rosario, ya que tenemos todos los datos encontrados dentro del rectángulo de la bounding box. Para “recortar” los datos conservando solo las calles de la ciudad, podemos extraer su intersección con el polígono de límites que obtuvimos antes. calles &lt;- st_intersection(calles, bbox_poly) Ahora si! ggplot() + geom_sf(data = calles) Podemos visualizar atributos de las calles, por ejemplo el de la velocidad máxima permitida, que está presente para casi todas. Pero antes va a ser necesario limpiar un poco los datos… como es usual. Los dataframes en formato sf que crea osmdata tienen todos los valores en formato texto, incluso aquellos que son números como maxspeed (la velocidad máxima), o lanes, la cantidad de carriles. Lo arreglamos: calles &lt;- calles %&gt;% mutate(maxspeed = as.numeric(maxspeed), lanes = ifelse(is.na(lanes), 1, as.numeric(lanes))) Con eso tenemos limpias las variables de velocidad máxima y ancho en carriles. Listos para visualizar. ggplot(calles) + geom_sf(aes(color = maxspeed), alpha = 0.5) + scale_color_viridis_c() + theme_void() + labs(title = &quot;Rosario&quot;, subtitle = &quot;Vías de circulación&quot;, caption = &quot;fuente: OpenStreetMap&quot;, color = &quot;velocidad máxima&quot;) O podemos revisar la posición de las avenidas: ggplot() + geom_sf(data = calles, color = &quot;gray40&quot;, alpha = .5) + geom_sf(data = filter(calles, str_detect(name, &quot;Avenida&quot;)), color = &quot;salmon&quot;) + theme_void() + labs(title = &quot;Rosario&quot;, subtitle = &quot;Avenidas&quot;, caption = &quot;fuente: OpenStreetMap&quot;) 3.2 Un ejercicio más: ¡Bares en el barrio! Imaginemos que estamos interesados en identificar y caracterizas los bares presentes en un barrio determinado, como San Telmo en la Ciudad de Buenos Aires. Como punto de partida, podemos consultar la base de OSM a ver que encontramos. Comenzamos por definir nuestra área de interés bbox_st &lt;- getbb(&#39;San Telmo, Ciudad Autonoma de Buenos Aires&#39;) bbox_st_poly = getbb(&#39;San Telmo, Ciudad Autonoma de Buenos Aires&#39;, format_out = &quot;sf_polygon&quot;) leaflet(bbox_st_poly) %&gt;% addTiles() %&gt;% addPolygons() Habiendo verificado que tenemos el área correcta, armamos una consulta por la grilla de calles, y la ejecutamos. SanTelmo_calles &lt;- opq(bbox_st) %&gt;% add_osm_feature(key = &quot;highway&quot;) %&gt;% osmdata_sf() Y también descargamos información sobre la posición de bares. Habiendo revisado https://wiki.openstreetmap.org/wiki/Map_Features, sabemos que para obtener bares necesitamos la categoría “amenity”, y el subtipo “bar”. En términos de OSM, key = &quot;amenity&quot;, value = &quot;bar&quot;: SanTelmo_bares &lt;- opq(bbox_st) %&gt;% add_osm_feature(key = &quot;amenity&quot;, value = &quot;bar&quot;) %&gt;% osmdata_sf() Extraemos la información dentro de los límites exactos del barrio. A diferencia de las calles, que aparecen en la geometría de líneas, para los bares nos interesan los puntos. SanTelmo_calles &lt;- st_intersection(SanTelmo_calles$osm_lines, bbox_st_poly) SanTelmo_bares &lt;- st_intersection(SanTelmo_bares$osm_points, bbox_st_poly) Y listos para mapear! De paso, resaltamos aquellos donde se baila tango, al colorear según el atributo “dance.style”, incluido en los datos. ggplot() + geom_sf(data = SanTelmo_calles, color = &quot;darkslateblue&quot;) + geom_sf(data = SanTelmo_bares, aes(color = dance.style)) + geom_sf_label(data = SanTelmo_bares, aes(label = name), size = 2) + theme_void() + labs(title = &quot;San Telmo&quot;, subtitle = &quot;Bares&quot;, caption = &quot;fuente: OpenStreetMap&quot;, color = &quot;Ofrecen baile&quot;) Casi listo. Antes de darnos por satisfechos, tenemos que mejorar la ubicación de las etiquetas, que se superponen por la proximidad de los lugares. Por el momento geom_sf_label() -la geometría de ggplot que permite graficar etiquetas de datos sf- no incluye la útil opción de correr la posición de las etiquetas en forma automática para que no se solapen. Por suerte, existe un pequeño paquete, ggsflabel, que provee la funcionalidad que necesitamos. Podemos instalar el paquete directo desde el repositorio de su autor: install.packages(&quot;devtools&quot;) devtools::install_github(&quot;yutannihilation/ggsflabel&quot;) library(ggsflabel) Y ahora, usamos geom_sf_label_repel() para la versión final de nuestro mapa de bares en San Telmo: ggplot() + geom_sf(data = SanTelmo_calles, color = &quot;darkslateblue&quot;) + geom_sf(data = SanTelmo_bares, aes(color = dance.style)) + geom_sf_label_repel(data = SanTelmo_bares, aes(label = name), size = 2) + theme_void() + labs(title = &quot;San Telmo&quot;, subtitle = &quot;Bares&quot;, caption = &quot;fuente: OpenStreetMap&quot;, color = &quot;Ofrecen baile&quot;) 3.3 Ejercicios Explorando y mapeando información georreferenciada de OpenStreetMap I. Descargar de OpenStreetMap la grilla de calles para la Ciudad elegida en el capítulo 2 (Geoprocesamiento) y mapearla por uno de sus atributos (velocidad mínima, velocidad máxima, cantidad de carriles, etc). Descargar de OpenStreetMap una (o más) capas de datos de tipo puntos o polígonos. Ver catálogo de categorías en este link Proyectar los datos descargados en un mapa y comentar los resultados: ¿Cómo se distribuyen en la Ciudad? Hacer un conteo de los ítems de la capa descargada por barrio, mapearlo y compararlo con el conteo de los ítems descargados en el ejercicio anterior: ¿La distribución es similar o hay diferencias? ¿A qué se puede deber? "],
["obteniendo-y-analizando-datos-de-redes-sociales.html", "Capítulo 4 Obteniendo y analizando datos de redes sociales 4.1 Investigando con social media 4.2 Conectando R a Twitter 4.3 Escuchando tweets en tiempo real 4.4 Capturando tweets por períodos prolongados 4.5 Capturando tweets en zonas específicas 4.6 Visualizando los datos georeferenciados 4.7 Ejercicios", " Capítulo 4 Obteniendo y analizando datos de redes sociales 4.1 Investigando con social media Los contenidos generados en las redes sociales son producidos y recopilados por usuarios individuales (o representeantes de organizaciones) que participan en plataformas de acceso público tales como Twitter, Facebook, o Instagram. Si bien esas tres son las más populares, existen muchísimas otras plataformas que funcionan como repositorios de información online, como Yelp (reseñas de restoranes) o Properati (listados de propiedades en venta y alquiler), entre tantos otros. La información producida en redes sociales llama la atención de investigadores en temas sociales por el nivel de detalle que encierra. Los usuarios registran y transmiten en forma pública un amplio abanico de datos personales: su paradero, estado de ánimo, intenciones futuras, etc. Es por eso que la “minería” de datos capturados online se utiliza para estudiar procesos sociales, políticos, y hasta meteorológicos (monitoreando menciones a eventos climáticos). 4.1.1 Los desafíos de trabajar con información de repositorios sociales Por supuesto, no todo son ventajas. El análisis de datos extraidos de redes sociales se enfrenta a varios obstáculos, entre ellos: “Suciedad” de los datos. Los contenidos publicados en redes sociales suelen combinar texto, imágenes y video, lo cual requiere un esfuerzo considerable para identificar y clasificar los tipos de contenido disponibles. Incluso el contenido más fácil de tratar, el texto, requiere de limpieza previa: hay que lidiar con abreviaciones, emojis, puntuación inusual, etc. Inconsistencia. Los regisros capturados desde repositorios online suelen ser inconsistentes: Muchas veces, uno o más de los valores de sus atributos (tales como “usario”, “mensaje”, “idioma”, etc) faltan en muchos de los registros. Por ejemplo, algunos contienen coordenadas espaciales que permiten ubicarlos en el espacio, pero en muchos casos no están georreferenciados. Eso dificulta saber dónde está siendo producida la información, desde donde se emite. Sesgo y veracidad dudosa. Al analizar los datos, es tentador realizar inferencias acerca de lo que la población en general hace o quiere. Pero hay que tener en cuenta que las personas que producen contenidos online son un grupo particular, que tiende a ser más joven y de nivel socioeconómico mayor a la media. Por otra parte, que algo se haya dicho online dista mucho de ser cierto o sincero. Ni siquiera podemos asumir que los usuarios son individuos humanos; las redes sociales son utilizadas en creciente medida por “bots”, software automático que publica contenidos en gran volumen simulando ser una persona, o un grupo de personas, con el fin de manipular la opinión pública. Volumen. Cuando uno decide acumular los datos que obtiene de redes sociales, durante meses o años, el volumen alcanzado no es trivial. Resguardar, ordenar, clasificar y extraer sentido de decenas o centenas de millones de registros es un desafío de big data. Limitaciones de acceso: Las empresas que controlan los repositorios de datos producidos en redes sociales vigilan con cuidado a quienes acceden, y limitan la cantidad de información que puede extraerse. En el caso de Twitter, las consultas permitidas a su base de datos se limitan al contenido producido en la última semana, y no entrega más de 18,000 tweets por consulta. 4.2 Conectando R a Twitter Para acceder a los sistemas de Twitter necesitamos obtener una autorización, identificándonos con nuestro usuario en la red. Este paso es inevitable, ya que sin una autorización Twitter no responderá nuestras consultas. Por suerte, el trámite para obtener acceso es inmediato, y como resultado obtendremos una serie de códigos de identificación, conocidos en su conjunto como API keys. 4.2.1 Obteniendo autorización El primer paso es, si no lo hemos hecho aún, crear un usuario de Twitter en https://twitter.com/signup. Luego seguimos los pasos de éste instructivo https://towardsdatascience.com/access-data-from-twitter-api-using-r-and-or-python-b8ac342d3efe. Nota: Twitter nos preguntará cómo se llama la “app” para la cual estamos solicitand acceso. No vamos a crear ninguna app, pero aún así tenemos que elegir un nombre; usemos “RTWEET” (aunque podría ser cualquier otro). Al completar los pasos estaremos en poder de cuatro códigos: Consumer Key, Consumer Secret, Access Token y Access Token Secret. Tomemos nota de ellos. 4.2.2 Acceso a Twitter via R: el paquete rtweet rtweet provee un conjunto de funciones que nos facilitan interactuar con Twitter. Si no lo tenemos instalado, lo conseguimos vía: install.packages(&quot;rtweet&quot;) Y lo activamos junto al resto de los paquetes que vamos a usar. library(rtweet) library(tidyverse) A continuación, le pasamos a rtweet los datos de autorización que conseguimos antes para crear un “token” (en la jerga de Twitter, es una especie de comprobante de que estamos autorizados a acceder a los datos) # El nombre que le asgnamos a la app en el formulario de autorización appname &lt;- &quot;RTWEET&quot; ## consumer key (en el ejemplo no es una clave real, usar la verdadera) consumer_key &lt;- &quot;la_secuencia_de_caracteres_de_nuestra_consumer_key&quot; ## consumer secret (en el ejemplo no es un clave real, usar la verdadera) consumer_secret &lt;- &quot;la_secuencia_de_caracteres_de_nuestra_consumer_secret&quot; ## consumer key (en el ejemplo no es una clave real, usar la verdadera) access_token &lt;- &quot;la_secuencia_de_caracteres_de_nuestro_access_token&quot; ## consumer secret (en el ejemplo no es un clave real, usar la verdadera) access_secret &lt;- &quot;la_secuencia_de_caracteres_de_nuestro_access_secret&quot; twitter_token &lt;- create_token( app = appname, consumer_key = consumer_key, consumer_secret = consumer_secret, access_token = access_token, access_secret = access_secret) Al ejecutar esas líneas se abrirá una ventana en nuestro navegador solicitando autorizar el acceso vía R -lo aceptamos, por supuesto. Ahora si, estamos listos para realizar consultas en el archivo de Twitter. La función search_tweets() permite obtener tweets que cumplan los requisitos que fijemos. Por ejemplo, para buscar tweets que contienen el término “inflacion”, usamos: tweets &lt;- search_tweets(q = &quot;inflacion&quot;, n = 3000) El parámetro n = 3000 es para limitar la búsqueda a los primeros 3000 tweets hallados. También puede hacerse una búsqueda por múltiples términos. Por ejemplo, buscando “ciudad+universitaria” hace que Twitter devuelva resultados donde las palabras aparecen juntas y en ese orden; como alternativa, al optar por “ciudad universitaria” se obtienen tweets donde aparezcan esas palabras en cualquier parte del texto, sin importar su orden o si aparecen contiguas. El resultado es un dataframe de 3000 observaciones -el número máximo que habíamos solicitado- y 88 columnas. rtweet incluye users_data(), una función que muestra detalles de los usuarios que han producido los tweets que capturamos: users_data(tweets) %&gt;% head() ## # A tibble: 6 x 20 ## user_id screen_name name location description url protected followers_count ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;int&gt; ## 1 257472… pablosebas… Seba… &quot;Tucumá… &quot;Ingeniero… &lt;NA&gt; FALSE 279 ## 2 960382… PieNPi Pabl… &quot;Argent… &quot;Underdog-… &lt;NA&gt; FALSE 288 ## 3 106816… Abeldealme… Abel… &quot;El Bol… &quot;Profesor … &lt;NA&gt; FALSE 1471 ## 4 442015… JulianColo… Juli… &quot;Mar De… &quot;UNMDP Der… http… FALSE 1954 ## 5 837485… Yorsha7 Yors… &quot;Argent… &quot;Morocha A… &lt;NA&gt; FALSE 331 ## 6 102506… Sofia99473… Sofia &quot;&quot; &quot;&quot; &lt;NA&gt; FALSE 3 ## # … with 12 more variables: friends_count &lt;int&gt;, listed_count &lt;int&gt;, ## # statuses_count &lt;int&gt;, favourites_count &lt;int&gt;, account_created_at &lt;dttm&gt;, ## # verified &lt;lgl&gt;, profile_url &lt;chr&gt;, profile_expanded_url &lt;chr&gt;, ## # account_lang &lt;lgl&gt;, profile_banner_url &lt;chr&gt;, profile_background_url &lt;chr&gt;, ## # profile_image_url &lt;chr&gt; También podemos explorar los resultados en base a las 88 variables disponibles. Revisemos los nombres: names(tweets) ## [1] &quot;user_id&quot; &quot;status_id&quot; ## [3] &quot;created_at&quot; &quot;screen_name&quot; ## [5] &quot;text&quot; &quot;source&quot; ## [7] &quot;display_text_width&quot; &quot;reply_to_status_id&quot; ## [9] &quot;reply_to_user_id&quot; &quot;reply_to_screen_name&quot; ## [11] &quot;is_quote&quot; &quot;is_retweet&quot; ## [13] &quot;favorite_count&quot; &quot;retweet_count&quot; ## [15] &quot;quote_count&quot; &quot;reply_count&quot; ## [17] &quot;hashtags&quot; &quot;symbols&quot; ## [19] &quot;urls_url&quot; &quot;urls_t.co&quot; ## [21] &quot;urls_expanded_url&quot; &quot;media_url&quot; ## [23] &quot;media_t.co&quot; &quot;media_expanded_url&quot; ## [25] &quot;media_type&quot; &quot;ext_media_url&quot; ## [27] &quot;ext_media_t.co&quot; &quot;ext_media_expanded_url&quot; ## [29] &quot;ext_media_type&quot; &quot;mentions_user_id&quot; ## [31] &quot;mentions_screen_name&quot; &quot;lang&quot; ## [33] &quot;quoted_status_id&quot; &quot;quoted_text&quot; ## [35] &quot;quoted_created_at&quot; &quot;quoted_source&quot; ## [37] &quot;quoted_favorite_count&quot; &quot;quoted_retweet_count&quot; ## [39] &quot;quoted_user_id&quot; &quot;quoted_screen_name&quot; ## [41] &quot;quoted_name&quot; &quot;quoted_followers_count&quot; ## [43] &quot;quoted_friends_count&quot; &quot;quoted_statuses_count&quot; ## [45] &quot;quoted_location&quot; &quot;quoted_description&quot; ## [47] &quot;quoted_verified&quot; &quot;retweet_status_id&quot; ## [49] &quot;retweet_text&quot; &quot;retweet_created_at&quot; ## [51] &quot;retweet_source&quot; &quot;retweet_favorite_count&quot; ## [53] &quot;retweet_retweet_count&quot; &quot;retweet_user_id&quot; ## [55] &quot;retweet_screen_name&quot; &quot;retweet_name&quot; ## [57] &quot;retweet_followers_count&quot; &quot;retweet_friends_count&quot; ## [59] &quot;retweet_statuses_count&quot; &quot;retweet_location&quot; ## [61] &quot;retweet_description&quot; &quot;retweet_verified&quot; ## [63] &quot;place_url&quot; &quot;place_name&quot; ## [65] &quot;place_full_name&quot; &quot;place_type&quot; ## [67] &quot;country&quot; &quot;country_code&quot; ## [69] &quot;geo_coords&quot; &quot;coords_coords&quot; ## [71] &quot;bbox_coords&quot; &quot;status_url&quot; ## [73] &quot;name&quot; &quot;location&quot; ## [75] &quot;description&quot; &quot;url&quot; ## [77] &quot;protected&quot; &quot;followers_count&quot; ## [79] &quot;friends_count&quot; &quot;listed_count&quot; ## [81] &quot;statuses_count&quot; &quot;favourites_count&quot; ## [83] &quot;account_created_at&quot; &quot;verified&quot; ## [85] &quot;profile_url&quot; &quot;profile_expanded_url&quot; ## [87] &quot;account_lang&quot; &quot;profile_banner_url&quot; ## [89] &quot;profile_background_url&quot; &quot;profile_image_url&quot; Allí hay de todo para explorar. 4.2.3 Usuarios más populares Según la cantidad de seguidores: options(scipen = 20) ggplot(tweets) + geom_histogram(aes(x = followers_count)) El gráfico muestra una distribución de “power law”, típica en los rankings de popularidad. Hay una enorme masa de usuarios con popularidad mínima (apenas un puñado de seguidores) y un número muy pequeño de usuarios que alcanza una cantidad deseguidores cientos o miles de veces superior a la de la mayoría. Obtenemos un top 5 de los usuarios más populares (con más seguidores), su procedencia, y el contenido del tweet: tweets %&gt;% top_n(5, followers_count) %&gt;% arrange(desc(followers_count)) %&gt;% select(screen_name, followers_count, location, text) ## # A tibble: 6 x 4 ## screen_name followers_count location text ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 la_patilla 7092240 Venezuela ALnavío: Así es el fracaso de Nic… ## 2 la_patilla 7092240 Venezuela ALnavío: Así es el fracaso de Nic… ## 3 la_patilla 7092240 Venezuela ALnavío: Así es el fracaso de Nic… ## 4 la_patilla 7092240 Venezuela ALnavío: Así es el fracaso de Nic… ## 5 clarincom 2959475 Buenos Aires, … Los ferroviarios cerraron la pari… ## 6 clarincom 2959475 Buenos Aires, … El ABL porteño aumentará mensualm… 4.2.4 Tweets más populares En base a la cantidad de retweets que recibieron. Nos quedamos sólo con los tweets originales, descartando los que son retweets en si mismos (“is_retweet == TRUE”), y revisamos la distribución de sus retweets: ggplot(filter(tweets, !is_retweet))+ geom_histogram(aes(x = retweet_count)) Otra ditribución power law. Identifiquemos el tweet original más que sumó más retweets: tweets %&gt;% filter(!is_retweet) %&gt;% filter(retweet_count == max(retweet_count)) %&gt;% select(screen_name, retweet_count, followers_count, location, text) ## # A tibble: 1 x 5 ## screen_name retweet_count followers_count location text ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 beltrandelr… 80 517300 &quot;México… Y los que no crean que lo… Nota: Si no estamos interesados en capturar retweets, podemos evitarlos al momento de consultar la base de Twitter, as’i tweets &lt;- search_tweets(q = &quot;inflacion&quot;, n = 500, include_rts = FALSE) 4.2.5 La hora del día a la que se producen los tweets rtweet() provee una funció que hace facil mostrar la evolución temporal de nuestros tweets: ts_plot(). Podemos ver la frecuencia de tweets por segundo, hora, día, semana, mes o año eligiendo el parámetro correspondiente (“seconds”, “minutes”, “hours”, “days”, “weeks”, “months”, o “years”) ts_plot(tweets, &quot;minutes&quot;) 4.2.6 Procedencia de los usuarios tweets %&gt;% ggplot() + geom_bar(aes(location)) + coord_flip() + labs(title = &quot;Procedencia de los usuarios&quot;, x = &quot;cantidad&quot;, y = &quot;ubicación&quot;) Dado que el campo “location” refleja el texto que cada usuario eligió para describir su ubicación (no se trata de las coordenadas de origen del tweet) las variabilidad es grande. Algunas escriben su país, otros su ciudad, otras su barrio… y hay quienes eligen opciones poéticas cómo “algún lugar del mundo”. En todo caso, la abundancia de opciones resulta en un gráfico muy difícil de leer. Probamos extraer el top 10 de lugares más frecuentes, eliminando los tweets de usuarios sin datos en su atributo “location”. tweets %&gt;% filter(location != &quot;&quot;, !is.na(location)) %&gt;% count(location) %&gt;% top_n(10, n) %&gt;% ggplot() + geom_col(aes(x = reorder(location, n), y = n)) + coord_flip() + labs(title = &quot;Procedencia de los usuarios&quot;, x = &quot;ubicación&quot;, y = &quot;cantidad&quot;) 4.3 Escuchando tweets en tiempo real Como alternativa a consultar el archivo “histórico” de Twitter, es posible conectar a su API de streaming, que entrega tweets en tiempo real al instante en que se producen. La función stream_tweets() permite iniciar una conexión y capturar tweets hasta que concluya el tiempo dispuesto por el parámetro “timeout”, expresado en segundos. Por ejemplo, para “escuchar” el stream de Twitter por un minuto (60 segundos), y capturar mensajes que incluyan los términos accidente y tránsito: captura_streaming &lt;- stream_tweets(q = &quot;accidente+tránsito&quot;, timeout = 60) Verificamos el resultado (sólo los campos de usuario y texto del tweet): captura_streaming[4:5] ## # A tibble: 3 x 2 ## screen_name text ## &lt;chr&gt; &lt;chr&gt; ## 1 stalinbriones Tremenda irresponsabilidad ## 2 3_xhamb3r buena farra!!! ## 3 talalata19 @sttmed @MovilidadEnv Por favor. Revisen accidente de tránsito … 4.4 Capturando tweets por períodos prolongados Cuando queremos monitorear un evento de actualidad, por ejemplo capturando tweets que mencionen una palabra o hashtag de interés, resulta necesario mantener las escucha activa durante varias horas o días. La solución para este caso es usar la función stream_tweets(), que permite iniciar un proceso de escucha de tiempo arbitrario. Dado que no se sabe que puede fallar en un proceso que dura varios días, la función se encarga de guardar los resultados en un archivo local a medida que se obtienen, y reiniciar la conexión a Twitter en forma automática si se interrumpe por algún motivo (como un corte momentáneo de acceso a internet). La usamos así: terminos_de_busqueda &lt;- &quot;accidente + tránsito&quot; # una semana: 60 segundos * 60 * 24 * 7 tiempo &lt;- 60 * 60 * 24 * 7 # El archivo donde guardar los resultados en disco (tendrá formato json, así que lo usamos en el nombre de archivo) archivo &lt;- &quot;busqueda_tweets.json&quot; stream_tweets(q = terminos_de_busqueda, timeout = tiempo, file_name = archivo, parse = FALSE) Una vez que el período de captura termina, podemos leer el archivo generado. # en el paso anterior definimos que el nombre de archivo es &quot;busqueda_tweets_DT_VP.json&quot; tweets &lt;- parse_stream(&quot;busqueda_tweets.json&quot;) Y con eso concluye el proceso. Ya estamos listos para analizar el contenido. 4.5 Capturando tweets en zonas específicas Imaginemos ahora que queremos obtener tweets originados en un lugar en particular. En un barrio, una ciudad, o un país. Para ello podemos aprovechar que Twitter permite realizar búsquedas por área geográfica. Por ejemeplo, iniciemos la descarga de tweets que mencionen el nombre que se le da en Buenos Aires al metro: “subte”. La clave está en que además de los términos de búsqueda vamos a especificar un radio de 20 millas (~32 km) en torno al área céntrica (el downtown) de la Ciudad: tweets_transporte &lt;- search_tweets(q = &quot;subte&quot;, geocode = &quot;-34.603722,-58.381592,20mi&quot;, include_rts = FALSE, n = 100000, retryonratelimit = TRUE) El proceso puede tomar un rato. Quien no pueda esperar, puede descargar unos resultados obtenidos previamente: tweets_transporte &lt;- readRDS(url(&quot;https://bitsandbricks.github.io/data/tweets_transporte.RDS&quot;)) 4.5.1 Extraer las coordenadas Algunos de los tweets, aquellos que fueron publicados desde un teléfono móvil u otro dispositivo con GPS, tienen coordenadas de posición precisas (latitud y longitud). El dataframe creado por rtweet guarda la pareja de coordenadas em el campo “coords_coords”, dentro de una lista. Es decir que en lugar de un valor simple, cada elemento de la columna contiene una lista de varios valores. De manera similar, también crea otras dos columnas, “geo_coords” y “bbox_coords” que contienen datos sobre la ubicación del tweet en forma de lista. Esto trae dos problemas: No podemos usar los verbos de manipulación de datos con esas columnas (filter, mutate, arrange, etc) porque están diseñados para trabajar con valores atómicos, como “hola” y no listas, como (“hola”, “chau”, “sin datos”). No podemos guardar el dataframe en formato .csv, el favorito de los que comparten datos, porque no hay forma estandarizada de indicar que algunos campos contienen una lista de datos en lugar de un valor único. write.csv y write_csv intentan guardar el dataframe en un archivo .csv, pero fallan al encontrar la primera columna que contiene listas. La solución es simple: usamos la función lat_lng(), que agrega al dataframe dos columnas adicionales llamadas “lat” y “lng”, conteniendo latitud y longitud para los tweets que traen posición exacta. tweets_transporte &lt;- lat_lng(tweets_transporte) Además, si quisiéramos guardar luego los datos en formato .csv, podemos descartar los campos problemáticos -los que contienen información geográfica en forma de listas-. La función select() nos permite retirarlos: tweets_transporte &lt;- tweets_transporte %&gt;% select(-geo_coords, -coords_coords, -bbox_coords) Para trabajar con los tweets georefernciados como un conjunto aparte, filtramos nuestra base para conservar sólo los mensajes que contienen coordenadas exactas de posición. tweets_transporte_geo &lt;- tweets_transporte %&gt;% filter(!is.na(lat), !is.na(lng)) El resultado evidencia que los tweets georeferenciados son sólo una fracción del total que se produce: nrow(tweets_transporte_geo) ## [1] 202 A continuación: ¡veamos los tweets en el mapa! 4.6 Visualizando los datos georeferenciados 4.6.1 Mapas estáticos con ggmap ggmap es un paquete de R que complementa a ggplot, agregando funciones que permiten adquirir y visualizar mapas en forma fácil. Si no lo tenemos instalado, ya sabemos que hacer: install.packages(&quot;ggmap&quot;) Lo activamos: library(ggmap) Ahora, para obtener un mapa base del área donde se encuentran los puntos que queremos mostrar, necesitamos determinar la “bounding box”: el rango de latitud y longitud que forma un rectángulo conteniendo todas las posiciones. En resumidas cuentas, se trata de los valores de latitud máxima y mínima, y de longitud máxima y mínima. Los proveedores de mapas online suelen solicitar los valores en este orden: izquierda, abajo, derecha, arriba. Es decir, posición mas al oeste, posición mas al sur, posición mas al este, posición mas al norte. Cuando disponemos de un dataframe con columnas de latitud y longitud, obtener la bounding box es bastante fácil: bbox &lt;- make_bbox(lon = tweets_transporte_geo$lng, lat = tweets_transporte_geo$lat) bbox ## left bottom right top ## -58.82968 -34.84405 -58.22878 -34.42486 Con eso podemos descargar un mapa del área. Como opción por defecto, ggmap solicita los mapas a Google Maps, pero esta ha dejado de ser la alternativa ideal: desde octubre de 2018, Google exige a los usarios registrarse y proveer una tarjeta de crédito para descargar información mediante porgramas propios. Por eso vamos a usar otra de las fuentes habilitadas por ggmap, el servicio de mapas de Stamen Design. Lo descargamos entregando la bounding box del área que nos interesa y un nivel de zoom. El nivel de zoom -un número de 1 a 20- indica el detalle que tendrá el mapa descargado. Para un área metropolitana un zoom de entre 10 y 12 es adecuado. mimapa &lt;- get_stamenmap(bbox, zoom = 11) Para ver el resultado usamos ggmap(): ggmap(mimapa) Stamen ofrece varios estilos de mapa: “terrain” (usado por defecto), “terrain-background”, “terrain-labels”, “terrain-lines”, “toner”, “toner-2010”, “toner-2011”, “toner-background”, “toner-hybrid”, “toner-labels”, “toner-lines”, “toner-lite”, “watercolor”. Probemos algunos: mimapa_terrain_lines &lt;- get_stamenmap(bbox, maptype = &quot;terrain-lines&quot;, zoom = 11) mimapa_toner_lite &lt;- get_stamenmap(bbox, maptype = &quot;toner-lite&quot;, zoom = 11) mimapa_watercolor &lt;- get_stamenmap(bbox, maptype = &quot;watercolor&quot;, zoom = 11) ggmap(mimapa_terrain_lines) ggmap(mimapa_toner_lite) ggmap(mimapa_watercolor) Cuando descargamos un mapa que vamos a usar de base para visualizar datos, siempre es una buena idea elegir una opción en escala de grises, sin colores que compitan contra los datos que proyectaremos. Probamos entonces con “toner-lite” para el mapa que usaremos de aqui en adelante. mapa_BA &lt;- get_stamenmap(bbox, maptype = &quot;toner-lite&quot;, zoom = 11) ggmap(mapa_BA) Ahora agregamos capas de puntos mostrando la posición de los tweets. La sintaxis es la misma que aprendimos para ggplot; de hecho, ggmap es una llamada a ggplot que tras bambalinas se encarga de los ajustes necesarios para mostrar el mapa como fondo. Habiendo revisado la data de Moreno, sabemos que las columnas de longitud y latitud de los puntos georeferenciados se llaman “lon” y “lat”. Al graficar los puntos, las usaremos como posición x e y respectivamente. ggmap(mapa_BA) + geom_point(data = tweets_transporte_geo, aes(x = lng, y = lat)) También podemos asignar a cada punto un color de acuerdo a la popularidad del usuario: ggmap(mapa_BA) + geom_point(data = tweets_transporte_geo, aes(x = lng, y = lat, color = followers_count)) + scale_color_distiller(palette = &quot;Spectral&quot;) ¿Qué pasó aquí? Tenemos un escala de colores que llega hasta 600.000, en el tono rojo, pero en el mapa solo vemos puntos azules, los que indican una cantidad baja de seguidores. La explicación está en la relativa rareza de usuarios de Twitter con cientos de miles de seguidores. Dado que la inmensa mayoría de usuarios de la red sólo tienen un puñado de seguidores, ocurre que puntitos que los representan suelen tapar a los esporádicos usuarios ultra populares. Si lo que queremos es mostrar los tweets de éstos últimos, podemos recurrir a un pequeño truco. Dado que las filas de un data frame se grafican en orden, si ordenamos las observaciones en orden creciente de “followers_count” los usuarios populares serán graficados al final, garantizando que su color aparezca por encima de otros. tweets_transporte_geo &lt;- arrange(tweets_transporte_geo, followers_count) ggmap(mapa_BA) + geom_point(data = tweets_transporte_geo, aes(x = lng, y = lat, color = followers_count)) + scale_color_distiller(palette = &quot;Spectral&quot;) Tambien podemos usar el tamaño de cada punto para representar la repercusión de los tweets, en base a la cantidad de “retweets” que han obtenido: ggmap(mapa_BA) + geom_point(data = tweets_transporte_geo, aes(x = lng, y = lat, color = followers_count, size = retweet_count), alpha = .5) + scale_color_distiller(palette = &quot;Spectral&quot;) 4.6.2 Mapas interactivos con leaflet Con la explosión de de popularidad de los mapas online, con Google Maps al frente, se ha vuelto habitual explorar información geográfica en entornos interactivos, que permiten al usuario desplazarse libremente por la superficie terrestre y cambiar el nivel de zoom con el que se muestran los datos. Un mapa con información tan precisa como la posición de los tweets, que incluso permite ver a parcela desde donde se han emitido, se beneficia en extremo de la posibilidad de variar la escala de visualización a voluntad. Desde R es fácil proyectar nuestros datos sobre mapas interactivos, usando el paquete leaflet. Si aún no lo tenemos en nuestro sistema, lo obtenemos mediante: install.packages(&quot;leaflet&quot;) Una vez que está instalado, lo activamos library(leaflet) EL uso de leaflet es similar al de ggplot; uno toma un dataframe y lo muestra mediante capas que exponen distintos aspectos de la información. Para empezar, hacemos leaflet(tweets_transporte_geo) … y no obtuvimos mucho. Tal como pasa con ggplot(), si uno no define ninguna capa de visualización, el resultado es una especie de lienzo vacío. Siguiente paso: agregar un mapa base. Para sumar capas a un mapa de leaflet usamos &quot; %&gt;% &quot; en ugar del &quot; + &quot; que requiere ggplot(), pero el concepto es el mismo. leaflet(tweets_transporte_geo) %&gt;% addTiles() Ahora está un poco mejor, nos encontramos con un mapa, pero falta que aparezcan nustros datos. Es fácil: con addMarkers() leaflet se encarga de buscar las coordenadas de cada observación, y si aparecen con algún nombre esperable, las identifica y sitúa en el mapa un pin por cada una. Nombres esperables serían “latitude” y “longitude” o también, como en nuestro caso, “lat” y “lng”. Si las coordenadas aparecieran bajo columnas con nombres menos interpretables, se le puede aclarar a leaflet cuáles son vía paramétros. leaflet(tweets_transporte_geo) %&gt;% addTiles() %&gt;% addMarkers() Ya tenemos un mapa útil! Para mejorarlo, agregamos la opción de “popup”, que permite extraer información adicional cliqueando sobre un pin. Por ejemplo, el contenido del campo con el texto de cada tweet (nótese el uso de “~”, que leaflet requiere para entender que nos referimos a un campo presente en el dataframe que le pasamos). leaflet(tweets_transporte_geo) %&gt;% addTiles() %&gt;% addMarkers(popup = ~text) Para sumar una dimensión más a la visualización, podemos usar el color para indicar la cantidad de seguidores del autor de cada tweet. Para codificar por color, leaflet requiere definir una paleta de colores para aplicar a nuestros datos. Al crear una paleta debemos elegir la función correspondiente al tipo de datos que vamos a mostrar: colorFactor() para variables categóricas, colorNumeric() para variabes numéricas, o colorQuantile() también para variables numéricas, pero agrupadas en cuantiles. La función requiere al menos dos parámetros. Uno es “palette”, para definir los tonos a usar. Aquí funcionan nuestros amigos viridis, magma, plasma e inferno, y también las paletas Brewer, como _“Spectral_ o Accent). El parametro restante es”domain&quot;, que simplemente toma un vector con los datos que vamos a representar con la paleta. Como la cantidad de seguidores es una variable numérica, podemos usar: paleta &lt;- colorNumeric( palette = &quot;viridis&quot;, domain = tweets_transporte_geo$followers_count) Y luego la usamos en nuestro mapa: leaflet(tweets_transporte_geo) %&gt;% addTiles() %&gt;% addCircleMarkers(popup = ~text, color = ~paleta(followers_count)) Como siempre es muy util agregar una leyenda que explique la codificación de los datos. leaflet sólo permite mostrar leyendas basadas en color (no en el diamétro de los círculos), pero algo es algo. Agregamos la leyenda así: leaflet(tweets_transporte_geo) %&gt;% addTiles() %&gt;% addCircleMarkers(radius = ~retweet_count, popup = ~text, color = ~paleta(followers_count)) %&gt;% addLegend(title = &quot;seguidores&quot;, pal = paleta, values = ~followers_count) Esto es sólo una introducción a la producción de mapas interactivos. Para acceder a un recorrido por muchas otras opciones disponibles con leaflet, podemos visitar https://rstudio.github.io/leaflet/ 4.7 Ejercicios Capturando y explorando datos de Twitter I. Descargar tweets que se originen en los alrededores de la Ciudad con la que trabajaron en los capítulos anteriores, y explorar las columnas/variables que contiene. Analizar: ¿Cuáles son los mensajes con más repercusión? ¿Qué dicen? ¿En qué momento del día se realiza la mayor cantidad de tweets? Graficar. ¿Cómo se distribuye la popularidad de los usuarios? ¿Quiénes son los 5 que más seguidores tienen? Graficar. Aislando los tweets que poseen coordenadas geográficas (lat y long), crear mapas que muestren posición de los tweets y cantidad de seguidores del usuario que tuitea. "],
["analizando-dinámicas-espacio-temporales.html", "Capítulo 5 Analizando dinámicas espacio-temporales 5.1 Mirando al espacio 5.2 Ejercicios", " Capítulo 5 Analizando dinámicas espacio-temporales Para entender datasets con datos en gran volumen que poseen atributos de posición y tiempo, es útil visualizar el ritmo en el que ocurren (diario, mensual, anual, etc) y la forma en la que se disrtibuyen en el espacio. Para prácticar, trabajaremos con un dataset de delitos registrados en la Ciudad de Buenos Aires. Los datos fueron publicados por el Gobierno de la Ciudad como parte de http://mapa.seguridadciudad.gob.ar/, y luego recopilados en un repositorio descargable disponible en https://github.com/ramadis/delitos-caba. delitos &lt;- read.csv(&quot;https://bitsandbricks.github.io/data/crimenydelito.csv&quot;, stringsAsFactors = FALSE) Chequeamos un resumen del contenido del dataset: summary(delitos) ## id comuna barrio latitud ## Min. : 1 Length:184879 Length:184879 Min. :-34.71 ## 1st Qu.: 46220 Class :character Class :character 1st Qu.:-34.63 ## Median : 92440 Mode :character Mode :character Median :-34.61 ## Mean : 92440 Mean :-34.59 ## 3rd Qu.:138660 3rd Qu.:-34.59 ## Max. :184879 Max. : 0.00 ## longitud fecha hora uso_arma ## Min. :-58.53 Length:184879 Length:184879 Length:184879 ## 1st Qu.:-58.47 Class :character Class :character Class :character ## Median :-58.44 Mode :character Mode :character Mode :character ## Mean :-58.40 ## 3rd Qu.:-58.40 ## Max. : 0.00 ## uso_moto lugar origen_dato tipo_delito ## Length:184879 Length:184879 Mode:logical Length:184879 ## Class :character Class :character NA&#39;s:184879 Class :character ## Mode :character Mode :character Mode :character ## ## ## ## cantidad_vehiculos cantidad_victimas ## Min. : 0.0000 Min. :0.000000 ## 1st Qu.: 0.0000 1st Qu.:0.000000 ## Median : 0.0000 Median :0.000000 ## Mean : 0.2081 Mean :0.001168 ## 3rd Qu.: 0.0000 3rd Qu.:0.000000 ## Max. :12.0000 Max. :2.000000 Mirando el resumen nos enteramos de que la hora más habitual para un delito son las 8 de la noche, que el delito más frecuente es el hurto sin violencia, y que el lugar más habitual donde ocurren los incidentes es en la vía pública, entre otras cosas. 5.0.1 Trabajando con fechas La fecha es un tipo de dato que puede ser expresado de muchas maneras, dependiendo de que nos interese tener en cuenta: el día de la semana al que corresponde, el mes, el año, etc. El paquete lubridate hace fácil extraer fechas en cualquier formato (por ejemplo “20/07/2018”) o el atributo relacionado que deseemos (como “viernes” o “Julio”). Para empezar, convertimos el campo “fecha” al tipo de dato especializado que se llama… fecha (date). Aquí tenemos que prestar atención al formato en que aparecen, en general algo como “2018-07-21” (mes, día y año) o “2018-07-21 12:14:24” (mes, día, año y hora, minutos, segundos). Con nuestros datos se da el primer caso, por lo cual la función para convertir ese campo en fecha es ymd(); para el segundo caso, seria ymd_hms() library(tidyverse) library(lubridate) delitos &lt;- delitos %&gt;% mutate(fecha = ymd(fecha)) Repasemos algunas de los nuevos trucos que podemos hacer con el tiempo. Tomemos cinco fechas elegidas al azar: set.seed(&quot;99&quot;) muestra_de_fechas &lt;- delitos %&gt;% mutate(fecha_hora = paste(fecha, hora)) %&gt;% sample_n(5) %&gt;% pull(fecha_hora) muestra_de_fechas ## [1] &quot;2016-07-07 20:00:00&quot; &quot;2016-09-07 13:25:00&quot; &quot;2016-04-09 15:00:00&quot; ## [4] &quot;2016-08-12 00:00:00&quot; &quot;2016-01-24 00:00:00&quot; Mediante las funciones diponibles en lubridate, podemos extraer: El día de la semana al que corresponde cada fecha: wday(muestra_de_fechas) ## [1] 5 4 7 6 1 wday(muestra_de_fechas, label = TRUE) ## [1] jue mié sáb vie dom ## Levels: dom &lt; lun &lt; mar &lt; mié &lt; jue &lt; vie &lt; sáb El mes: month(muestra_de_fechas) ## [1] 7 9 4 8 1 month(muestra_de_fechas, label = TRUE) ## [1] jul sep abr ago ene ## 12 Levels: ene &lt; feb &lt; mar &lt; abr &lt; may &lt; jun &lt; jul &lt; ago &lt; sep &lt; ... &lt; dic El año: year(muestra_de_fechas) ## [1] 2016 2016 2016 2016 2016 Y varias opciones más, que se pueden repasar en https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html Con lo visto hasta aquí, tenemos suficiente para mostrar patrones temporales en los datos. Delitos registrados por año: options(scipen = 20) ggplot(delitos) + geom_bar(aes(x = year(fecha))) Los resultados no permiten comparar entre años, ya que el dataset tiene apenas un puñado de registros en el 2015, y sólo llega a mediados del 2017. Lección: quedémonos sólo con el 2016 y miremos dentro. delitos %&gt;% filter(year(fecha) == 2016) %&gt;% ggplot() + geom_bar(aes(x = month(fecha, label = TRUE))) Se ve bastante parejo! Quizás haya que examinar los delitos por tipo, para ver si hay algunos que muestran altibajos según la estación. Veamos el top 5 de delitos por frecuencia. delitos %&gt;% count(tipo_delito) %&gt;% top_n(5) %&gt;% arrange(desc(n)) ## tipo_delito n ## 1 Robo (Con violencia) 90393 ## 2 Hurto (Sin violencia) 41205 ## 3 Lesiones Seg Vial 12791 ## 4 Hurto De Rueda 9904 ## 5 Hurto Automotor 9144 Luce razonable. Guardamos la lista de delitos más frecuentes para referenciar luego. delitos_frecuentes &lt;- delitos %&gt;% count(tipo_delito) %&gt;% top_n(5) %&gt;% pull(tipo_delito) Y ahora los comparamos. En gráfico de barras “apilado”: delitos %&gt;% filter(year(fecha) == 2016, tipo_delito %in% delitos_frecuentes) %&gt;% ggplot() + geom_bar(aes(x = month(fecha, label = TRUE), fill = tipo_delito)) … y sin apilar delitos %&gt;% filter(year(fecha) == 2016, tipo_delito %in% delitos_frecuentes) %&gt;% ggplot() + geom_bar(aes(x = month(fecha, label = TRUE), fill = tipo_delito), position = &quot;dodge&quot;) O como líneas: # Primero realizamos un conteo de delitos por tipo y por mes del año conteo &lt;- delitos %&gt;% filter(year(fecha) == 2016, tipo_delito %in% delitos_frecuentes) %&gt;% count(tipo_delito, mes = month(fecha, label = TRUE)) # Y ahora a mostras las cantidades mensuales como líneas ggplot(conteo) + geom_line(aes(x = mes, y = n, group = tipo_delito, color = tipo_delito)) Ésta última opción es sin dudas la más clara, tanto para mostrar la diferencia relativa en el volumen de incidentes, como para indicar si existen fluctuaciones. Intentémoslo otra vez, ahora con el día de la semana: # Primero realizamos un conteo de delitos por tipo y por día de la semana conteo &lt;- delitos %&gt;% filter(year(fecha) == 2016, tipo_delito %in% delitos_frecuentes) %&gt;% count(tipo_delito, diasemana = wday(fecha, label = TRUE)) # Y ahora a mostras las cantidades mensuales como líneas ggplot(conteo) + geom_line(aes(x = diasemana, y = n, group = tipo_delito, color = tipo_delito)) Como algunas categorías están mucho menos representadas, quedan “aplastadas” en el gráfico con lo que se dificulta su legibilidad. Vamos a comparar porcentajes en lugar de valores absolutos. conteo &lt;- conteo %&gt;% group_by(tipo_delito) %&gt;% mutate(pct = n / sum(n) * 100) ggplot(conteo) + geom_line(aes(x = diasemana, y = pct, group = tipo_delito, color = tipo_delito)) La diferencia de volumen según el día parece drástica, pero es engañosa: el eje de las \\(y\\) no empieza en 0, lo cual hace que las diferencias se perciban mayores de lo que son. Forzamos al gráfico a comenzar desde 0 el eje \\(y\\): ggplot(conteo) + geom_line(aes(x = diasemana, y = pct, group = tipo_delito, color = tipo_delito))+ expand_limits(y = 0) Sin dudas, los domingos son el día en que el delito descansa un poco… excepto pra quienes deciden llevarse un auto o al menos una rueda ajena. El pico de delitos reportados ocurre los viernes, con la excepción del hurto de rueda, que crece de domingo a miércoles y luego decae. Recordando que los homicidios suelen tener un contexto muy distinto al de los robos, agreguemos la categoría “Homicidio Doloso” para comparar con las demás. conteo_homicidios &lt;- delitos %&gt;% filter(year(fecha) == 2016, tipo_delito == &quot;Homicidio Doloso&quot;) %&gt;% count(tipo_delito, diasemana = wday(fecha, label = TRUE)) %&gt;% group_by(tipo_delito) %&gt;% mutate(pct = n / sum(n) *100) Sumamos la nueva categoría: ggplot(conteo) + geom_line(aes(x = diasemana, y = pct, group = tipo_delito, color = tipo_delito)) + geom_line(data = conteo_homicidios, aes(x = diasemana, y = pct, group = tipo_delito)) + labs(title = &quot;Distribución diaria por tipo de delito&quot;, subtitle = &quot;La línea negra representa homicidios&quot;, x = &quot;día&quot;, y = &quot;%&quot;, color = &quot;Delitos más frecuentes&quot;) + expand_limits(y = 0) La categoría homicidio muestra un ritmo inverso al de los otros delitos: es más frecuente durante el fin de semana, decayendo en los días hábiles. También podemos evaluar el ritmo según la hora del día. Para ello necesitamos pasar a formato temporal la columna “hora”, que en éste dataset tiene el formato “hh:mm:ss” (por ejemplo, “14:55:00”). La función correspondiente para interpretar ese formato es hms(). Usamos una combinacion de hms() para interpretar el texto en “hora” como una variable de tipo tiempo, y hour() para extraer la hora base -por ejemplo, para “19:55:00” la hora base es 19. por_hora &lt;- delitos %&gt;% filter(year(fecha) == 2016, tipo_delito %in% delitos_frecuentes) %&gt;% count(tipo_delito, hora_base = hour(hms(hora))) %&gt;% group_by(tipo_delito) %&gt;% mutate(pct = n / sum(n) *100) ggplot(por_hora) + geom_line(aes(x = hora_base, y = pct, group = tipo_delito, color = tipo_delito)) + labs(title = &quot;Distribución horaria por tipo de delito&quot;, x = &quot;hora&quot;, y = &quot;%&quot;, color = &quot;Delitos más frecuentes&quot;) + expand_limits(y = 0) + scale_x_continuous(breaks = 0:23) Para los delitos relacionados con automóviles, el “prime time” es como el de la TV, de 20 a 22. Los robos violentos tienen su apogeo durante las 20. Las lesiones viales comparten su hora pico con el tráfico. Los hurtos tienen su momento cúlmine a la hora del almuerzo, y a la de salida de la oficina. 5.1 Mirando al espacio Pasemos ahora al análisis espacial de nuestros datos. Para facilitar la visualización vamos a usar el paquete ggmap, que incluye varias funciones que facilitan la creación de mapas. library(ggmap) 5.1.1 Obteniendo un mapa base Para obtener un mapa de fondo o “mapa base” necesitamos obtener una “bounding box” de nuestros datos, que luego pasamos a get_stamenmap(), como ya hicimos en el capítulo anterior cuando mapeamos tweets. Como suele ocurrir, hace falta un poco de limpieza previa: nos quedamos solo con los puntos ubicados en el hemisferio donde se ubica Buenos Aires, y con eso eliminamos algunos puntos mal localizados. delitos &lt;- delitos %&gt;% filter(latitud &lt;0, longitud &lt;0) Con los datos depurados, usamos la funcion make_bbox() que nos devuelve el rectángulo de coordenadas que contiene todas las ubicaciones en nuestra base: bbox &lt;- make_bbox(delitos$longitud, delitos$latitud) bbox ## left bottom right top ## -58.54060 -34.71439 -58.33150 -34.52541 Y en base a la “bounding box” solicitamos nuestro mapa base: CABA &lt;- get_stamenmap(bbox = bbox, maptype = &quot;toner-lite&quot;, zoom = 12) Para verlo: ggmap(CABA) 5.1.2 De coordenadas al mapa De aquí en más podemos suporponer nuestros datos en distintas capas, con la misma sintaxis que conocemos de ggplot. Para mapear las ubicaciones de los delitos en el dataset, usamos geom_point() y los campos de longitud y latitud para los ejes \\(x\\) e \\(y\\): ggmap(CABA) + geom_point(data = delitos, aes(x = longitud, y = latitud)) Aquí nos topamos con un problema, habitual al trabajar con grandes volúmenes de datos. Hay tantos puntos proyectados sobre el mapa, que se hace imposible interepretar donde existen más o menos. Hacemos algunos ajustes: un color más llamativo, un tamaño de punto más pequeño, y aplicación de una ligera transparencia, vía los atributos “color”, “size” y “alpha”. ¿Cuál es el valor ideal para cada uno? En general, no queda otra que recurrir a la prueba y error para encontrar la receta justa. ggmap(CABA) + geom_point(data = delitos, aes(x = longitud, y = latitud), color = &quot;orange&quot;, size = 0.1, alpha = 0.1) Ahora si aparecen ciertos patrones, por ejemplo la afinidad del delito con las grandes vías de circulación de la ciudad. Aún así, se hace dificil identificar de un golpe de vsta las “zonas calientes”, los puntos de máxima concentración. 5.1.3 Mapas de densidad Una solución práctica para el problema de la cantidad de puntos es una técnica llamada “binning”: dividir el espacio en una grilla de celdas, contar cuantos puntos caen dentro de cada una, y visualizar las cantidades agregadas. Hacerlo es muy fácil vía geom_bind2d(). ggmap(CABA) + geom_bin2d(data = delitos, aes(x = longitud, y = latitud)) Ahora si, resaltan las áreas de mayor concentración de incidentes. Se puede mejorar un poco el gráfico usando una mayor cantidad de celdas para aumentar la resolución. También empleando una escala de colores diseñada para ayudar a detectar diferencias por tonalidad, como viridis. ggmap(CABA) + geom_bin2d(data = delitos, aes(x = longitud, y = latitud), bins = 100) + scale_fill_viridis_c() Una alternativa al binning es la llamada kernel density estimation, muy utilizada en aplicaciones GIS para estimar la intensidad de una determinada variable en cualquier punto del área analizada, incluso en aquellos donde no hay observaciones. La idea es asumir que los valores observados corresponden a una distribución continua sobre el espacio, y determinar cual es la más probable en base a los puntos con datos. No hace falta realizar ningún cálculo matemático, sólo usar geom_density2d así: ggmap(CABA) + geom_density2d(data = delitos, aes(x = longitud, y = latitud, color = stat(level))) + scale_color_viridis_c() 5.1.4 Visualizando multiples categorías Hasta aquí hemos analizado la distribución espacial del delito en su totalidad, sin diferenciar su tipo. Veamos ahora las diferencias por categoría. Podemos reintentar el mapa de puntos, esta vez filtrando los tipos de delito para incluir sólo los más frecuentes, y diferenciándolos por color. ggmap(CABA) + geom_point(data = filter(delitos, tipo_delito %in% delitos_frecuentes), aes(x = longitud, y = latitud, color = tipo_delito), size = 0.1, alpha = 0.1) Aquí tenemos dos problemas: La leyenda (“tipo_delito”) es difícil de leer, dado que muestra los puntos tal como los definimos: pequeños y con mucha transparencia. Esos atributos son útiles en el mapa, donde tenemos cientos de miles de puntos, pero muy poco prácticos para la leyenda, donde sólo hay uno por etiqueta. Los puntos sobre el mapa se superponen en tal medida que es dificil discernir patrones espaciales distintos según su categoría. El primer problema se resuelve fijando “a mano” los atributos de la leyenda, asi: ggmap(CABA) + geom_point(data = filter(delitos, tipo_delito %in% delitos_frecuentes), aes(x = longitud, y = latitud, color = tipo_delito), size = 0.1, alpha = 0.1) + guides(color = guide_legend(override.aes = list(size=2, alpha = 1))) + scale_color_brewer(palette = &quot;Set1&quot;) El segundo, usando facetado para mostrar en su propio mapa a cada categoría: ggmap(CABA) + geom_point(data = filter(delitos, tipo_delito %in% delitos_frecuentes), aes(x = longitud, y = latitud, color = tipo_delito), size = 0.1, alpha = 0.1) + scale_color_brewer(palette = &quot;Set1&quot;) + facet_wrap(~tipo_delito) El facetado ayuda. Se hace evidente, por ejemplo, que el patrón espacial de las lesiones en seguridad vial es muy distinto al de hurto automotor. Para hacer las diferencias aún mas nítidas, podemos facetar una estimación de densidad: ggmap(CABA) + geom_density2d(data = filter(delitos, tipo_delito %in% delitos_frecuentes), aes(x = longitud, y = latitud, color = stat(level))) + scale_color_viridis_c() + facet_wrap(~tipo_delito) 5.1.5 Combinando espacio y tiempo El facetado también nos permite visualizar el cambio de posición a través del tiempo. Por ejemplo, podemos comparar dos tipos de delito (hurto sin violencia y hurto de rueda) mostrando dónde ocurren en cada día de la semana. delitos &lt;- delitos %&gt;% mutate(dia_semana = wday(fecha, label = TRUE)) ggmap(CABA) + geom_point(data = filter(delitos, tipo_delito %in% c(&quot;Hurto (Sin violencia)&quot;, &quot;Hurto De Rueda&quot;)), aes(x = longitud, y = latitud, color = tipo_delito), alpha = .5, size = .2) + facet_wrap(~dia_semana) O concentrarnos en un tipo de delito en particular, y evaluar en que zonas se concentra de acuerdo a la hora del día: delitos &lt;- delitos %&gt;% mutate(hora_base = hour(hms(hora))) ggmap(CABA) + geom_density2d(data = filter(delitos, tipo_delito == &quot;Hurto (Sin violencia)&quot;, !(wday(fecha) %in% 2:5) ), aes(x = longitud, y = latitud, color = stat(level))) + scale_color_viridis_c() + facet_wrap(~hora_base, nrow = 4) + labs(title = &quot;Concentración espacial de hurtos&quot;, subtitle = &quot;según hora del día&quot;) 5.2 Ejercicios Descubriendo patrones temporales y espaciales en los datos I. Utilizar los tweets que se descargaron en el capítulo anterior o elegir algún dataset open data de la Ciudad que tenga tanto coordenadas como fecha. Quienes se animen pueden trabajar con ambos y luego compararlos entre sí. Realizar 2 gráficos que les permitan analizar la temporalidad de los datos. ¿Detectan algún patrón temporal? ¿A qué puede deberse? Analizar la distribución espacial de los datos a partir de: Un mapa de densidad que muestre donde se concentran la mayor cantidad de observaciones. Un facetado del mapa del punto III.a. que permita discernir patrones espaciales dentro de las categorías de una variable. ¿El patrón espacial de los datos elegidos se mantiene o varía según las categorías? Comparar la densidad de los datos en el tiempo (facetar). ¿Los patrones espaciales de los datos elegidos se mantienen o varían en el tiempo? "],
["analizando-movimiento-el-flujo-de-viajes-urbanos.html", "Capítulo 6 Analizando movimiento: el flujo de viajes urbanos 6.1 Estimando rutas 6.2 Cuantificando interacción 6.3 Estimando rutas 6.4 EXTRA: Cómo obtener las rutas de todos los recorridos 6.5 Ejercicios", " Capítulo 6 Analizando movimiento: el flujo de viajes urbanos Los sistemas urbanos se caracterizan por dinámicas continuas de flujo, como el viaje de las personas entre su lugar de trabajo y de residencia. Estas dinámicas son capturadas en diversas bases de datos con creciente grado de granularidad espacio-temporal. La disponibilidad de coordenadas precisas de origen y destino, combinada con la posibilidad de acceder a sistemas de ruteo en calles, nos permite estimar los trayectos realizado por personas y vehículos representados en bases de datos. 6.1 Estimando rutas En general, los datos de flujo disponibles en datasets a escala metropolitana (en contraste con los datos personales como los de GPS) son simples pares origen/destino. Una ejemplo de datos abiertos de este tipo, es el de la ubicación e intercambio entre estaciones de sistemas de bicicletas compartidas. Por ejemplo, el portal de datos abiertos de la Ciudad de Buenos Aires ofrece datasets con los trayectos realizados por los usuarios del sistema de bicicletas públicas, así como la ubicación de las estaciones. Si no lo hemos hecho aún, carguemos las librerías que vamos a necesitar. library(tidyverse) library(ggmap) Utilizaremos una porción de todos los trayectos disponibles, los que representan viajes en bicicletas públicas realizados durante el mes de abril de 2017: viajes &lt;- read_csv(&quot;https://bitsandbricks.github.io/data/viajes_BA_bici_abril_2017.csv&quot;) viajes ## # A tibble: 113,650 x 6 ## HORA ORIGEN_ESTACION NOMBRE_ORIGEN DESTINO_ESTACION ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2017-04-01 00:00:00 1 FACULTAD DE … 42 ## 2 2017-04-01 00:00:00 5 PLAZA ITALIA 14 ## 3 2017-04-01 00:00:00 5 PLAZA ITALIA 20 ## 4 2017-04-01 00:00:00 5 PLAZA ITALIA 69 ## 5 2017-04-01 00:00:00 5 PLAZA ITALIA 94 ## 6 2017-04-01 00:00:00 5 PLAZA ITALIA 123 ## 7 2017-04-01 00:00:00 6 PARQUE LEZAMA 17 ## 8 2017-04-01 00:00:00 6 PARQUE LEZAMA 28 ## 9 2017-04-01 00:00:00 6 PARQUE LEZAMA 118 ## 10 2017-04-01 00:00:00 8 CONGRESO 1 ## # … with 113,640 more rows, and 2 more variables: NOMBRE_DESTINO &lt;chr&gt;, ## # TOTAL &lt;dbl&gt; También descargamos un archivo de información geográfica con la posición de cada estación de bicicletas públicas: estaciones &lt;- read_csv(&quot;https://bitsandbricks.github.io/data/estaciones_BA_bici.csv&quot;) estaciones ## # A tibble: 199 x 10 ## X Y NOMBRE DOMICILIO IMAGEN AUTOMAT OBSERV NRO_EST HORARIO DIRE_NORM ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 -58.4 -34.6 FACULT… AV. PRES… ESTAC… AUTOMA… ABRIL… 1 ESTACI… FIGUEROA… ## 2 -58.4 -34.6 RETIRO AV. DR.J… ESTAC… AUTOMA… ABRIL… 2 ESTACI… DEL LIBE… ## 3 -58.4 -34.6 ADUANA AV. ING.… ESTAC… AUTOMA… ABRIL… 3 ESTACI… HUERGO, … ## 4 -58.4 -34.6 PLAZA … LAVALLE … ESTAC… AUTOMA… ABRIL… 4 ESTACI… LAVALLE … ## 5 -58.4 -34.6 PARQUE… AV MARTI… ESTAC… AUTOMA… ABRIL… 6 ESTACI… GARCIA, … ## 6 -58.4 -34.6 PLAZA … AV. SANT… ESTAC… AUTOMA… ABRIL… 5 ESTACI… SARMIENT… ## 7 -58.4 -34.6 OBELIS… AV. 9 DE… ESTAC… AUTOMA… ABRIL… 7 ESTACI… PELLEGRI… ## 8 -58.4 -34.6 CONGRE… AV. HIPO… ESTAC… AUTOMA… ABRIL… 8 ESTACI… YRIGOYEN… ## 9 -58.4 -34.6 PARQUE… PARQUE L… ESTAC… AUTOMA… ABRIL… 9 ESTACI… DIAZ, CN… ## 10 -58.4 -34.6 PUERTO… MOREAU D… ESTAC… AUTOMA… ABRIL… 10 ESTACI… 1500 MOR… ## # … with 189 more rows Ahora, las visualizamos. Como preparativo obtenemos una “bounding box”, la caja de coordenadas que contiene todos los puntos: bbox &lt;- make_bbox(estaciones$X, estaciones$Y) bbox ## left bottom right top ## -58.46000 -34.64541 -58.35132 -34.56439 Ahora descargamos un mapa que abarca el rectángulo de nuestra bounding box mapa_base &lt;- get_stamenmap(bbox, color = &quot;bw&quot;, zoom = 12) ggmap(mapa_base) + geom_point(data = estaciones, aes(x = X, y = Y), color = &quot;limegreen&quot;) Podemos ver que las estaciones del sistema se concentran en el centro económico de la ciudad y sus zonas aledañas. No tenemos un campo con la fecha de inauguración que nos permita saber el orden en que se desplegaron las estaciones, pero podemos usar el número que les fue asignado (asumiendo que respetan un orden cronológico) para aproximarlo: ggmap(mapa_base) + geom_point(data = estaciones, aes(x = X, y = Y, color = NRO_EST)) + scale_color_distiller(type = &quot;div&quot;) Si el número de estación refleja la antigüedad, pareciera que primero de desplegó un corredor desde el downtown hacia el noroeste, que luego se fue complementando con expansión radial. 6.2 Cuantificando interacción A partir de ahora, agreguemos theme_nothing() para retirar todos los componentes auxiliares (como escalas y leyendas) y quedarnos solo con el mapa. ggmap(mapa_base) + geom_point(data = estaciones, aes(x = X, y = Y), color = &quot;limegreen&quot;, size = 2) + theme_nothing() A continuación, realizamos un conteo de trayectos entre pares de estaciones conteo &lt;- viajes %&gt;% group_by(ORIGEN_ESTACION, DESTINO_ESTACION) %&gt;% summarise(total = sum(TOTAL)) Podemos evaluar el grado de interconexión haciendo un heatmap, un mapa de calor que muestre la cantidad de viajes entre pares de estaciones. Hacemos uso de geom_tile() una geometría de ggplot() que genera rectángulos. ggplot() + geom_tile(data = conteo, aes(x = ORIGEN_ESTACION, y = DESTINO_ESTACION, fill = total)) + scale_fill_distiller(palette = &quot;Spectral&quot;) El gráfico revela una característica de los datos: la numeración de la estaciones es discontinua. Crece secuencialmente hasta casi 200, pero por alguna razón hay un par de estaciones numeradas por encima de 500. Lo verificamos: unique(conteo$ORIGEN_ESTACION) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 16 17 18 19 ## [19] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 ## [37] 38 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 ## [55] 57 58 59 60 61 62 63 64 65 66 68 69 70 71 72 73 74 75 ## [73] 76 77 79 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 ## [91] 96 98 99 100 101 103 104 105 106 108 109 110 111 112 113 114 115 118 ## [109] 119 120 121 122 123 124 126 128 129 132 134 135 136 138 139 140 144 145 ## [127] 146 149 150 151 152 153 154 158 160 161 164 166 167 172 173 175 176 181 ## [145] 191 502 505 Podemos evitar el hueco que aparece en el mapa de calor tratando a las estaciones como una variable categórica (un factor) en lugar de numérica ggplot() + geom_tile(data = conteo, aes(x = as.factor(ORIGEN_ESTACION), y = as.factor(DESTINO_ESTACION), fill = total)) + scale_fill_distiller(palette = &quot;Spectral&quot;) La visualización es difícil de leer, pero aún así revela patrones. El tipo de viaje más popular es el de tomar y dejar la bicicleta en la misma estación, sugiriendo la prevalencia del uso recreativo. La interacción entre estaciones más alta se da entre las que tienen los primeros números, que como hemos visto se localizan en el centro de la ciudad. Las cantidad de combinaciones posibles crece rapidísimo con el número de nodos, por eso las interacción en redes grandes es difícil de visualizar. Para continuar, tomemos sólo los 10 trayectos más frecuentes, descartando los viajes “circulares” (con el mismo origen y destino): top10 &lt;- conteo %&gt;% ungroup() %&gt;% filter(ORIGEN_ESTACION != DESTINO_ESTACION) %&gt;% top_n(10) top10 ## # A tibble: 10 x 3 ## ORIGEN_ESTACION DESTINO_ESTACION total ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 333 ## 2 1 44 146 ## 3 2 1 311 ## 4 2 10 181 ## 5 2 164 115 ## 6 9 44 121 ## 7 10 2 182 ## 8 30 9 118 ## 9 44 1 176 ## 10 44 14 127 ggplot() + geom_tile(data = top10, aes(x = as.factor(ORIGEN_ESTACION), y = as.factor(DESTINO_ESTACION), fill = total)) + scale_fill_distiller(palette = &quot;Spectral&quot;) Como se vislumbra en el heatmap completo, la interacción entre las estaciones 1 y 2 es con diferencia la más frecuente. 6.3 Estimando rutas Para trazar los trayectos de los usuarios al viajar de una estación a otra, no tendría sentido tender líneas rectas entre origen y destino. Para visualizar el tránsito, necesitamos tener en cuenta la ubicación de las calles y la dirección de tráfico que permiten. Lo ideal sería poder representar la ruta exacta de cada trayecto, sabiendo cuáles fueron las calles transitadas para realizar el viaje. Cuando no disponemos de información con ese nivel de detalle, lo que podemos hacer es estimar los recorridos utilizando un servicio de ruteo como el de Google Maps, o el del proyecto OSRM. En R contamos con paquetes especializados para conectar con estos servicios y trabajar con información de ruteo. El paquete googleway() permite conectar R con la API de Google Maps, y osrm hace lo propio con OSRM. Vamos con OSRM. Si no tenemos el paquete necesario, lo instalamos. install.packages(&quot;osrm&quot;) Y lo activamos: library(osrm) Para poder recibir información de ruteo desde los servidores de Google, la compañía exige el uso de una API key, una clave de autorización. Tal como con Twitter, el proceso de adquirir una clave es instantáneo, pero desde mediados de 2018 Google entrega API key sólo a usuarios que brinden información de una tarjeta de crédito, para cobrar el uso que supere ciertos umbrales. Para quienes deseen hacer uso de las múltiples funciones que Google ofrece a través de sus APIs, la molestia vale la pena, y puede seguir éstos pasos: https://developers.google.com/maps/documentation/directions/get-api-key. Para resolver el problema del ejercicio, nosotros optaremos por el ruteo vía OSRM que no requiere permiso ni tarjetas de crédito. Para encontrar una ruta, usamos la función osrmRoute, que requiere origen y destino en forma de vectores conteniendo un identificador (nombre del lugar o posición), longitud y latitud. Por ejemplo, para rutear entre dos lugares en Buenos Aires como Parque Centenario y la Estación Retiro: pcentenario &lt;- c(nombre = &quot;Parque Centenario&quot;, lon = -58.435609, lat = -34.606411) eretiro &lt;- c(nombre = &quot;Estación Retiro&quot;, lon = -58.374873, lat = -34.591394) centenario_a_retiro &lt;- osrmRoute(src = pcentenario, dst = eretiro, returnclass = &quot;sf&quot;, overview = &quot;full&quot;) La opción returnclass = &quot;sf&quot; permite obtener un dataframe espacial como resultado, que podemos proyectar luego sobre un mapa. overview = &quot;full&quot; hace que osrmRoute calcule la ruta precisa (con posiciones exactas) en lugar de un aproximado; de nuevo, solicitamos esto para luego poder visualizar el camino exacto en un mapa. osrmRoute también estima la duración (en minutos) y la distancia (en kilómetros) del trayecto, como se ve en los campos “duration” y “distance”: centenario_a_retiro ## Simple feature collection with 1 feature and 4 fields ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: -58.43689 ymin: -34.60636 xmax: -58.37422 ymax: -34.59099 ## CRS: EPSG:4326 ## src dst duration ## Parque Centenario_Estación Retiro Parque Centenario Estación Retiro 84.83333 ## distance geometry ## Parque Centenario_Estación Retiro 6.9386 LINESTRING (-58.43598 -34.6... Podemos revisar rápidamente la ruta hallada usando leaflet: library(leaflet) leaflet(centenario_a_retiro) %&gt;% addTiles() %&gt;% addPolylines(color = &quot;red&quot;) Ahora, lo intentamos con los datos de viajes en bicicleta. Hacemos un join del dataframe con el conteo de viajes contra el de posición de estaciones, para agregar las coordenadas. De origen: top10 &lt;- top10 %&gt;% left_join(estaciones[c(&quot;X&quot;, &quot;Y&quot;, &quot;NOMBRE&quot;, &quot;NRO_EST&quot;)], by = c(&quot;ORIGEN_ESTACION&quot; = &quot;NRO_EST&quot;)) %&gt;% rename(ORIGEN_X = X, ORIGEN_Y = Y, ORIGEN_NOMBRE = NOMBRE) top10 ## # A tibble: 10 x 6 ## ORIGEN_ESTACION DESTINO_ESTACION total ORIGEN_X ORIGEN_Y ORIGEN_NOMBRE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 2 333 -58.4 -34.6 FACULTAD DE DERECHO ## 2 1 44 146 -58.4 -34.6 FACULTAD DE DERECHO ## 3 2 1 311 -58.4 -34.6 RETIRO ## 4 2 10 181 -58.4 -34.6 RETIRO ## 5 2 164 115 -58.4 -34.6 RETIRO ## 6 9 44 121 -58.4 -34.6 PARQUE LAS HERAS ## 7 10 2 182 -58.4 -34.6 PUERTO MADERO - UCA ## 8 30 9 118 -58.4 -34.6 PEÑA ## 9 44 1 176 -58.4 -34.6 ZOOLOGICO ## 10 44 14 127 -58.4 -34.6 ZOOLOGICO Y además las de destino: top10 &lt;- top10 %&gt;% left_join(estaciones[c(&quot;X&quot;, &quot;Y&quot;, &quot;NOMBRE&quot;, &quot;NRO_EST&quot;)], by = c(&quot;DESTINO_ESTACION&quot; = &quot;NRO_EST&quot;)) %&gt;% rename(DESTINO_X = X, DESTINO_Y = Y, DESTINO_NOMBRE = NOMBRE) top10 ## # A tibble: 10 x 9 ## ORIGEN_ESTACION DESTINO_ESTACION total ORIGEN_X ORIGEN_Y ORIGEN_NOMBRE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 2 333 -58.4 -34.6 FACULTAD DE … ## 2 1 44 146 -58.4 -34.6 FACULTAD DE … ## 3 2 1 311 -58.4 -34.6 RETIRO ## 4 2 10 181 -58.4 -34.6 RETIRO ## 5 2 164 115 -58.4 -34.6 RETIRO ## 6 9 44 121 -58.4 -34.6 PARQUE LAS H… ## 7 10 2 182 -58.4 -34.6 PUERTO MADER… ## 8 30 9 118 -58.4 -34.6 PEÑA ## 9 44 1 176 -58.4 -34.6 ZOOLOGICO ## 10 44 14 127 -58.4 -34.6 ZOOLOGICO ## # … with 3 more variables: DESTINO_X &lt;dbl&gt;, DESTINO_Y &lt;dbl&gt;, ## # DESTINO_NOMBRE &lt;chr&gt; Probemos rutear el trayecto más popular, el de Facultad de Derecho a Retiro: viaje &lt;- top10[1,] fderecho_a_retiro &lt;- osrmRoute(src = c(viaje$ORIGEN_NOMBRE, viaje$ORIGEN_X, viaje$ORIGEN_Y), dst = c(viaje$DESTINO_NOMBRE, viaje$DESTINO_X, viaje$DESTINO_Y), returnclass = &quot;sf&quot;, overview = &quot;full&quot;) fderecho_a_retiro ## Simple feature collection with 1 feature and 4 fields ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: -58.39231 ymin: -34.59263 xmax: -58.37486 ymax: -34.58301 ## CRS: EPSG:4326 ## src dst duration distance ## FACULTAD DE DERECHO_RETIRO FACULTAD DE DERECHO RETIRO 24.15667 2.002 ## geometry ## FACULTAD DE DERECHO_RETIRO LINESTRING (-58.39231 -34.5... leaflet(fderecho_a_retiro) %&gt;% addTiles() %&gt;% addPolylines(color = &quot;red&quot;) Si queremos ver el trayecto en un mapa estático, podemos usar ggmap() con geom_sf(): ggmap(mapa_base) + geom_point(data = estaciones, aes(x = X, y = Y), color = &quot;limegreen&quot;, size = 2) + geom_sf(data = fderecho_a_retiro, color = &quot;red&quot;, inherit.aes = FALSE) + theme_nothing() Calcular todos los recorridos y juntarlos en un sólo dataframe puede ser muy fácil o bastante engorroso, dependiendo de cuanta práctica tengamos en la automatización de tareas repetitivas. Por lo pronto, podemos descargar un dataset ya calculado con los recorridos detallados entre todas las estaciones de nuestro top 10: recorridos &lt;- st_read(&quot;https://bitsandbricks.github.io/data/recorridos_BA_bici.geojson&quot;) ## Reading layer `recorridos_BA_bici&#39; from data source `https://bitsandbricks.github.io/data/recorridos_BA_bici.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 10 features and 6 fields ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: -58.42635 ymin: -34.62158 xmax: -58.36573 ymax: -34.5627 ## CRS: 4326 recorridos ## Simple feature collection with 10 features and 6 fields ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: -58.42635 ymin: -34.62158 xmax: -58.36573 ymax: -34.5627 ## CRS: 4326 ## ORIGEN_ESTACION DESTINO_ESTACION src dst duration distance ## 1 1 2 1 2 16.005000 5.4228 ## 2 1 44 1 44 6.741667 2.7496 ## 3 2 1 2 1 7.198333 2.3073 ## 4 2 10 2 10 10.708333 3.8060 ## 5 2 164 2 164 11.688333 4.3380 ## 6 9 44 9 44 8.775000 2.8171 ## 7 10 2 10 2 11.023333 3.4170 ## 8 30 9 30 9 7.986667 2.0258 ## 9 44 1 44 1 13.831667 7.8749 ## 10 44 14 44 14 8.278333 2.4001 ## geometry ## 1 LINESTRING (-58.39256 -34.5... ## 2 LINESTRING (-58.39256 -34.5... ## 3 LINESTRING (-58.37493 -34.5... ## 4 LINESTRING (-58.37493 -34.5... ## 5 LINESTRING (-58.37493 -34.5... ## 6 LINESTRING (-58.40661 -34.5... ## 7 LINESTRING (-58.36598 -34.6... ## 8 LINESTRING (-58.39733 -34.5... ## 9 LINESTRING (-58.41484 -34.5... ## 10 LINESTRING (-58.41484 -34.5... Los que quieran espiar un método para compilar los recorrido por su cuenta, puede verlo al final del documento. Para poder asignar un color a cada recorrido, creamos un identificador único para diferenciarlos recorridos &lt;- recorridos %&gt;% mutate(ID = paste(ORIGEN_ESTACION, &quot;-&quot;, DESTINO_ESTACION)) Y ahora, al mapa: ggmap(mapa_base) + geom_sf(data = recorridos, aes(color = ID), inherit.aes = FALSE) + theme_nothing() Si queremos que el grosor de la línea represente la cantidad de veces que se realizó cada recorrido, primero agregamos la cantidad de viajes por recorrido, mediante el cruce con los datos que calculamos en “conteo”: recorridos &lt;- recorridos %&gt;% left_join(conteo) Y luego los usamos en el mapa: ggmap(mapa_base) + geom_sf(data = recorridos, aes(color = ID, size = total), alpha = 0.7, inherit.aes = FALSE) + theme_nothing() También podemos usar el color para indicar el volumen de viajes: ggmap(mapa_base, darken = 0.7) + geom_sf(data = recorridos, aes(color = total, group = ID), inherit.aes = FALSE, alpha = 0.7, size = 1.5) + scale_color_viridis_c(option = &quot;inferno&quot;) + theme_nothing() 6.4 EXTRA: Cómo obtener las rutas de todos los recorridos Tras leer el capítulo de 21 de R for Data Science, “iteration”, ésto debería tener sentido: obtener_recorrido &lt;- function(o_nombre, o_x, o_y, d_nombre, d_x, d_y) { ruta &lt;- osrmRoute(src = c(o_nombre, o_x, o_y), dst = c(d_nombre, d_x, d_y), returnclass = &quot;sf&quot;) cbind(ORIGEN_ESTACION = o_nombre, DESTINO_ESTACION = d_nombre, ruta) } argumentos &lt;- list(top10$ORIGEN_ESTACION, top10$ORIGEN_X, top10$ORIGEN_Y, top10$DESTINO_ESTACION, top10$DESTINO_X, top10$DESTINO_Y) recorridos &lt;- pmap(argumentos, obtener_recorrido) recorridos &lt;- reduce(recorridos, rbind) recorridos ## Simple feature collection with 10 features and 8 fields ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: -58.42635 ymin: -34.62158 xmax: -58.36573 ymax: -34.5627 ## CRS: 4326 ## ORIGEN_ESTACION DESTINO_ESTACION src dst duration distance ID total ## 1 1 2 1 2 16.005000 5.4228 1 - 2 333 ## 2 1 44 1 44 6.741667 2.7496 1 - 44 146 ## 3 2 1 2 1 7.198333 2.3073 2 - 1 311 ## 4 2 10 2 10 10.708333 3.8060 2 - 10 181 ## 5 2 164 2 164 11.688333 4.3380 2 - 164 115 ## 6 9 44 9 44 8.775000 2.8171 9 - 44 121 ## 7 10 2 10 2 11.023333 3.4170 10 - 2 182 ## 8 30 9 30 9 7.986667 2.0258 30 - 9 118 ## 9 44 1 44 1 13.831667 7.8749 44 - 1 176 ## 10 44 14 44 14 8.278333 2.4001 44 - 14 127 ## geometry ## 1 LINESTRING (-58.39256 -34.5... ## 2 LINESTRING (-58.39256 -34.5... ## 3 LINESTRING (-58.37493 -34.5... ## 4 LINESTRING (-58.37493 -34.5... ## 5 LINESTRING (-58.37493 -34.5... ## 6 LINESTRING (-58.40661 -34.5... ## 7 LINESTRING (-58.36598 -34.6... ## 8 LINESTRING (-58.39733 -34.5... ## 9 LINESTRING (-58.41484 -34.5... ## 10 LINESTRING (-58.41484 -34.5... 6.5 Ejercicios Analizando y visualizando flujos de viajes urbanos I. Elegir alguna de las siguientes opciones: Un dataset que contenga viajes origen-destino (por ejemplo bicicletas públicas) de la Ciudad con la que están trabajando. Un dataset que contenga servicios esenciales (hospitales, escuelas, comisarías, etc) de la Ciudad con la que están trabajando. En este caso deberán elegir un barrio céntrico y uno periférico del shape de barrios y calcular ambos centroides. Los datos pueden ser descargados del portal open data de la ciudad o de OSM y deben tener ubicación geográfica. Según la opción elegida en el punto I, resolver: Viajes origen-destino: Analizar la cantidad de viajes entre los puntos a partir de un mapa de calor (heatmap) y calcular los 10 recorridos más realizados. Describir los resultados obtenidos y hacer el siguiente mapa: Mapa con los ruteos de los 10 recorridos más realizados. Servicios esenciales: Estimar la distancia entre los centroides calculados en el punto I y los ítems que componen la capa descargada. Describir los resultados obtenidos y hacer los siguientes mapas: Mapa con los ruteos del centroide del barrio céntrico a los ítems de servicios esenciales elegido. Mapa con los ruteos del centroide del barrio periférico a los ítems de servicios esenciales elegido. Consejos: Si bajamos datos de OpenStreetMap -por ejemplo, escuelas de una ciudad- los obetenemos en formato sf. Un dataframe sf tiene sus coordenadas en una sola columna, llamada “geometry”. Pero que pasa si queremos los datos en dos columnas, lat y long, para aprovechar los ejemplos de funciones de ruteo que hemos visto? Muy simple. Si su dataframe en formato sf se llama “misdatos”, así es como obtienen columnas con lat y long: misdatos &lt;- misdatos %&gt;% mutate(lat = unlist(map(misdatos$geometry,1)), long = unlist(map(misdatos$geometry,2))) "],
["machine-learning-en-una-aplicación-urbana.html", "Capítulo 7 Machine Learning (en una aplicación urbana) 7.1 Paso 0: Cargar paquetes 7.2 Paso 1: Cargar los datos 7.3 Paso 2: Examinar los datos 7.4 Paso 3: Limpiar los datos 7.5 Paso 4: Crear sets de entrenamiento y de testeo 7.6 Ejercicios", " Capítulo 7 Machine Learning (en una aplicación urbana) El así llamado machine learning consiste el empleo de aprendizaje estadístico automatizado para identificar patrones en grandes volúmenes de datos. El machine learning (de aquí en más ML) es utilizado en infinidad de campos debido a su creciente facilidad de uso y capacidad -en ciertos contextos- para predecir resultados con alta precisión. A continuación veremos como se aplica ML para predecir el valor de venta de los departamentos en CABA a partir de un dataset publicado en el portal de datos abiertos BA Data que contiene el relevamiento de departamentos en venta que realizó el GCBA en 2016. El objetivo del ejercicio es predecir el valor del metro cuadrado (USD x m2) de los departamentos en función de atributos como la cantidad de m2 descubiertos, la cantidad de ambientes, el barrio donde se ubican, la antiguedad de la construcción, etc. Allá vamos. 7.1 Paso 0: Cargar paquetes Además de las funciones de R “base”, vamos a usar las del paquete tidyverse para procesar y visualizar nuestros datos, las de sf para hacer algunos análisis espaciales y las de randomForest, para aplicar el algoritmo de ML homónimo, que es relativamente simple y a la vez efectivo. library(tidyverse) library(sf) #install.packages(&quot;randomForest&quot;) library(randomForest) 7.2 Paso 1: Cargar los datos Descargamos de BA Data el dataset del relevamiento de departamentos en venta del siguiente modo: dptos_2016 &lt;- read.csv(&quot;http://cdn.buenosaires.gob.ar/datosabiertos/datasets/departamentos-en-venta/departamentos-en-venta-2016.csv&quot;, encoding=&quot;UTF-8&quot;, sep=&quot;;&quot;) 7.3 Paso 2: Examinar los datos Echamos un vistazo a los nombres de las columnas y las primeras filas del dataset: names(dptos_2016) ## [1] &quot;CALLE&quot; &quot;NUMERO&quot; ## [3] &quot;ID_ZONAPRO&quot; &quot;OPERACION&quot; ## [5] &quot;TIPO&quot; &quot;M2&quot; ## [7] &quot;M2CUB&quot; &quot;PRECIOTEXT&quot; ## [9] &quot;PRECIOARS&quot; &quot;PRECIOARSM&quot; ## [11] &quot;DOLARES&quot; &quot;U_S_M2&quot; ## [13] &quot;AMBIENTES&quot; &quot;ANTIGUEDAD&quot; ## [15] &quot;BAÑOS&quot; &quot;DIRECCION&quot; ## [17] &quot;LOCATION&quot; &quot;PUBLICADO&quot; ## [19] &quot;PROCESADO&quot; &quot;URL&quot; ## [21] &quot;REVISION&quot; &quot;NOTA&quot; ## [23] &quot;DIRECCION_NORMALIZADA&quot; &quot;BARRIO&quot; ## [25] &quot;COMUNA&quot; &quot;CODIGO_POSTAL&quot; ## [27] &quot;CODIGO_POSTAL_ARGENTINO&quot; &quot;LATITUD&quot; ## [29] &quot;LONGITUD&quot; head(dptos_2016) ## CALLE NUMERO ID_ZONAPRO OPERACION TIPO M2 M2CUB PRECIOTEXT ## 1 GUATEMALA 5574 42408691 VTA DTO 57 50 U$S 170.150 ## 2 ZAPATA 300 42408710 VTA DTO 46 46 U$S 118.650 ## 3 ZAPATA 300 42518390 VTA DTO 61 56 U$S 181.470 ## 4 ZAPATA 300 42518402 VTA DTO 140 76 U$S 320.000 ## 5 JUSTO, JUAN B. AV. 2300 42621693 VTA DTO 39 33 U$S 82.116 ## 6 JUSTO, JUAN B. AV. 2300 42621702 VTA DTO 39 34 U$S 81.921 ## PRECIOARS PRECIOARSM DOLARES U_S_M2 AMBIENTES ANTIGUEDAD BAÑOS ## 1 2977625 59553 170150 3403 2 2016 1 ## 2 2076375 45139 118650 2579 0 2016 1 ## 3 3175725 56709 181470 3241 2 2016 1 ## 4 5600000 73684 320000 4211 2 2016 1 ## 5 1437030 43546 82116 2488 0 2016 1 ## 6 1433617 42165 81921 2409 0 2016 1 ## DIRECCION ## 1 GUATEMALA 5574 ## 2 ZAPATA 300 ## 3 ZAPATA 300 ## 4 ZAPATA 300 ## 5 JUSTO JUAN B. AVDA. AL 2300 ## 6 JUSTO JUAN B. AVDA. AL 2300 ## LOCATION PUBLICADO ## 1 GUATEMALA 5574 PALERMO HOLLYWOOD PALERMO 27/11/2016 ## 2 ZAPATA 300 BELGRANO CAPITAL FEDERAL 27/11/2016 ## 3 ZAPATA 300 BELGRANO CAPITAL FEDERAL 27/11/2016 ## 4 ZAPATA 300 BELGRANO CAPITAL FEDERAL 27/11/2016 ## 5 JUSTO JUAN B. AVDA. AL 2300 VILLA CRESPO CAPITAL FEDERAL 27/7/2016 ## 6 JUSTO JUAN B. AVDA. AL 2300 VILLA CRESPO CAPITAL FEDERAL 27/7/2016 ## PROCESADO URL REVISION ## 1 12/10/2017 HTTP://WWW.ZONAPROP.COM.AR/PROPIEDADES/A-42408691.HTML NA ## 2 12/10/2017 HTTP://WWW.ZONAPROP.COM.AR/PROPIEDADES/A-42408710.HTML NA ## 3 11/10/2017 HTTP://WWW.ZONAPROP.COM.AR/PROPIEDADES/A-42518390.HTML NA ## 4 11/10/2017 HTTP://WWW.ZONAPROP.COM.AR/PROPIEDADES/A-42518402.HTML NA ## 5 10/10/2017 HTTP://WWW.ZONAPROP.COM.AR/PROPIEDADES/A-42621693.HTML NA ## 6 10/10/2017 HTTP://WWW.ZONAPROP.COM.AR/PROPIEDADES/A-42621702.HTML NA ## NOTA DIRECCION_NORMALIZADA BARRIO COMUNA CODIGO_POSTAL ## 1 NA GUATEMALA 5574 PALERMO COMUNA 14 1425 ## 2 NA ZAPATA 300 PALERMO COMUNA 14 1426 ## 3 NA ZAPATA 300 PALERMO COMUNA 14 1426 ## 4 NA ZAPATA 300 PALERMO COMUNA 14 1426 ## 5 NA JUSTO, JUAN B. AV. 2300 VILLA CRESPO COMUNA 15 1414 ## 6 NA JUSTO, JUAN B. AV. 2300 VILLA CRESPO COMUNA 15 1414 ## CODIGO_POSTAL_ARGENTINO LATITUD LONGITUD ## 1 C1425BVH -34.58058 -58.43176 ## 2 C1426AED -34.57387 -58.44061 ## 3 C1426AED -34.57387 -58.44061 ## 4 C1426AED -34.57387 -58.44061 ## 5 C1414CWY -34.59298 -58.44154 ## 6 C1414CWY -34.59298 -58.44154 El dataset contiene 29 columnas (mucha información!). Por lo tanto, debemos revisar las variables y hacer una preselección para incluir solo aquellas que consideremos relevantes para nuestro modelo: La variable a predecir (dependiente) será el valor del m2 de los departamentos (U_S_M2) en CABA. Las variables predictoras (independientes) serán: cantidad de ambientes (AMBIENTES), años de antiguedad de la construcción (ANTIGUEDAD), cantidad de baños (BAÑOS), superficie total (M2), superficie cubierta (M2CUB), barrio al que pertenece (BARRIO) y coordenadas (LATITUD, LONGITUD). Ahora si, seleccionemos únicamente las variables que queremos incluir: dptos_2016 &lt;- dptos_2016 %&gt;% select(M2, M2CUB, U_S_M2, AMBIENTES, ANTIGUEDAD, BAÑOS, BARRIO, LATITUD, LONGITUD) Y veamos un resumen del contenido: summary(dptos_2016) ## M2 M2CUB U_S_M2 AMBIENTES ## Min. : 15.00 Min. : 0.00 Min. : 0 Min. : 0.000 ## 1st Qu.: 41.00 1st Qu.: 37.00 1st Qu.: 2242 1st Qu.: 2.000 ## Median : 54.00 Median : 47.00 Median : 2651 Median : 2.000 ## Mean : 70.38 Mean : 61.12 Mean : 2804 Mean : 2.456 ## 3rd Qu.: 80.00 3rd Qu.: 70.00 3rd Qu.: 3131 3rd Qu.: 3.000 ## Max. :730.00 Max. :625.00 Max. :12500 Max. :10.000 ## ## ANTIGUEDAD BAÑOS BARRIO LATITUD ## Min. : 0.0 Min. :0.00 :1382 Min. :-34.68 ## 1st Qu.: 1.0 1st Qu.:1.00 CABALLITO : 643 1st Qu.:-34.62 ## Median : 30.0 Median :1.00 PALERMO : 629 Median :-34.60 ## Mean : 710.6 Mean :1.25 SAN CRISTOBAL: 563 Mean :-34.60 ## 3rd Qu.:2016.0 3rd Qu.:2.00 VILLA CRESPO : 532 3rd Qu.:-34.58 ## Max. :2016.0 Max. :6.00 BELGRANO : 473 Max. :-34.54 ## (Other) :3342 NA&#39;s :1382 ## LONGITUD ## Min. :-58.53 ## 1st Qu.:-58.45 ## Median :-58.43 ## Mean :-58.43 ## 3rd Qu.:-58.41 ## Max. :-58.35 ## NA&#39;s :1382 El resumen nos muestra que las superficies totales de los departamentos relevados varían entre 15 y 730m2, siendo 70m2 la media. También podemos ver que las variables M2CUB, U_S_M2, AMBIENTES, ANTIGUEDAD y BAÑOS tienen valores mínimos de 0, lo cual resulta bastante extraño. Pero no importa, no nos preocupemos porque ya aprendimos varias formas de limpiar datos, así que manos a la obra. 7.4 Paso 3: Limpiar los datos 7.4.1 Imputar valores faltantes Es habitual que los algoritmos empleados para ML no acepten datos faltantes. Es por eso que la limpieza básica de un dataset casi siempre incluye la imputación de datos no disponibles, evitando descartar por incompletas filas que contienen información valiosa en los campos que si están disponibles. Hasta acá pudimos observar varias inconsistencias en los datos cargados, como por ejemplo: La variable M2CUB tiene valor 0 en algunos registros. Suponiendo que hubo un error en la carga de los datos, cuando M2CUB&lt;15 vamos a imputar el valor de M2, asumiendo que esa propiedad no tiene m2 descubiertos. Hay casos donde M2CUB&gt;M2. Acá le imputaremos el valor del M2CUB al M2. En la variable ANTIGUEDAD aparecen algunos registros con el valor 2016. Suponiendo que esas propiedades se construyeron en ese año, se imputará una antiguedad 1 ya que fue construida en el mismo año del relevamiento. Del mismo modo, y bajo el mismo supuesto, todos los registos que tengan ANTIGUEDAD=0, serán reemplazados por ANTIGUEDAD=1. Las variables AMBIENTES y BAÑOS tiene 0 en algunos casos. Imputaremos estos datos entendiendo que cuando AMBIENTES=0, es un monoambiente, y que cuando BAÑOS=0, es porque tienen 1 solo. Hay 1.382 valores faltantes en las columnas LONGITUD y LATITUD, y a su vez estos registros tampoco tienen comuna o barrio asignado, por lo tanto como nos va a resultar imposible ubicarlos en el espacio, estos sí debemos eliminarlos. Por último, se puede ver que la variable a predecir (U_S_M2) varía entre 0 y 12500. Claramente ninguna propiedad publicada en CABA puede tener U_S_M2=0 o menos de 500 así que estos registros, que son pocos casos, también se eliminarán. Para llevar a cabo todos los ajustes mencionados, utilizaremos las ya conocidadas mutate y filter: dptos_2016 &lt;- dptos_2016 %&gt;% mutate(M2CUB=ifelse(M2CUB&lt;15, M2, M2CUB), M2=ifelse(M2CUB&gt;M2, M2CUB, M2), ANTIGUEDAD=ifelse(ANTIGUEDAD==2016, 1, ANTIGUEDAD), ANTIGUEDAD=ifelse(ANTIGUEDAD==0, 1, ANTIGUEDAD), BAÑOS = ifelse(BAÑOS==0, 1, BAÑOS), AMBIENTES = ifelse(AMBIENTES==0, 1, AMBIENTES)) %&gt;% filter(U_S_M2&gt;500, !is.na(LATITUD), !is.na(LONGITUD)) Listo, ya tenemos preparadas las variables para nuestro modelo, pero aún estamos a tiempo de generar algunas nuevas que consideremos que, por tener capacidad predictiva sobre el valor del m2, lo mejorarían. Por ejemplo, probemos sumar una nueva variable al modelo donde se calculen los metros descubiertos (M2DESC) de cada propiedad, ya que, es muy probable que tener alguna expansión (balcón o terraza) le de un valor agregado al departamento. dptos_2016 &lt;- dptos_2016 %&gt;% mutate(M2DESC=M2-M2CUB) Ahora sí, volvamos a ver el resumen: summary(dptos_2016) ## M2 M2CUB U_S_M2 AMBIENTES ## Min. : 15.00 Min. : 15.00 Min. : 608 Min. : 1.000 ## 1st Qu.: 41.00 1st Qu.: 38.00 1st Qu.: 2230 1st Qu.: 2.000 ## Median : 53.00 Median : 47.00 Median : 2631 Median : 2.000 ## Mean : 68.44 Mean : 60.28 Mean : 2768 Mean : 2.518 ## 3rd Qu.: 78.00 3rd Qu.: 69.00 3rd Qu.: 3107 3rd Qu.: 3.000 ## Max. :730.00 Max. :625.00 Max. :10555 Max. :10.000 ## ## ANTIGUEDAD BAÑOS BARRIO LATITUD ## Min. : 1.000 Min. :1.000 CABALLITO : 643 Min. :-34.68 ## 1st Qu.: 1.000 1st Qu.:1.000 PALERMO : 629 1st Qu.:-34.62 ## Median : 1.000 Median :1.000 SAN CRISTOBAL: 563 Median :-34.60 ## Mean : 9.535 Mean :1.331 VILLA CRESPO : 532 Mean :-34.60 ## 3rd Qu.: 10.000 3rd Qu.:2.000 BELGRANO : 472 3rd Qu.:-34.58 ## Max. :110.000 Max. :6.000 RECOLETA : 406 Max. :-34.54 ## (Other) :2933 ## LONGITUD M2DESC ## Min. :-58.53 Min. : 0.000 ## 1st Qu.:-58.45 1st Qu.: 0.000 ## Median :-58.43 Median : 4.000 ## Mean :-58.43 Mean : 8.163 ## 3rd Qu.:-58.41 3rd Qu.: 8.000 ## Max. :-58.35 Max. :282.000 ## Todo parece funcionar bien: Nos hemos librado de los NA y las inconsistencias que tenían los datos. Pero antes de seguir con nuestro modelo, espiemos la distribución de algunas variables, como por ejemplo el valor del m2: ggplot() + geom_histogram(data = dptos_2016, aes(x = U_S_M2)) La superficie total: ggplot() + geom_histogram(data = dptos_2016, aes(x = M2)) La superficie descubierta: ggplot() + geom_histogram(data = dptos_2016, aes(x = M2DESC)) La antigüedad de las viviendas: ggplot() + geom_histogram(data = dptos_2016, aes(x = ANTIGUEDAD)) Y el Barrio que, como se trata de una variable categórica en lugar de continua, lo veremos con un gráfico de barras en lugar de un histograma: ggplot() + geom_bar(data = dptos_2016, aes(x = BARRIO))+ theme(axis.text.x = element_text(size = 6, angle = 90)) Todavía podemos agregar una variable más: Probemos con la distancia de los departamentos a las estaciones de subte, ya que la cercanía a estas es muy probable que impacte en el valor del m2. Acá utilizaremos uno de los geoprocesos que aprendimos algunos capítulos atrás: st_distance Carguemos las estaciones: subte_estaciones &lt;- st_read(&quot;http://bitsandbricks.github.io/data/subte_estaciones.geojson&quot;) ## Reading layer `subte_estaciones&#39; from data source `http://bitsandbricks.github.io/data/subte_estaciones.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 86 features and 3 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -58.48639 ymin: -34.64331 xmax: -58.36993 ymax: -34.55564 ## CRS: 4326 Transformemos dptos_2016 a un dataset espacial para poder medir distancias: dptos_2016 &lt;- dptos_2016 %&gt;% st_as_sf(coords = c(&quot;LONGITUD&quot;, &quot;LATITUD&quot;), crs = 4326) Y ahora calculemos la distancia (en metros) entre cada departamento en venta y la estación de subte más cercana: dptos_2016 &lt;- dptos_2016 %&gt;% mutate(DIST_SUBTE = apply(st_distance(dptos_2016, subte_estaciones), 1, function(x) min(x))) Veamos un resumen de los resultados: summary(dptos_2016$DIST_SUBTE) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.35 282.77 477.19 706.81 762.33 5658.53 Y saquemos conclusiones: El departamento ubicado a menor distancia de alguna estación de subte es a 1.35 metros y el que está a mayor distancia es a 5658 metros (56 cuadras). Sin embargo, en promedio, las propiedades se ubican a 706 metros (7 cuadras) de alguna estación. En el paso anterior, transformamos nuestro dataset tradicional en un dataset espacial para poder medir distancias, pero como queremos utilizar los datos de LATITUD y LONGITUD para el modelo, debemos volver a separar las coordenadas y transformarlo en dataframe tradicional: dptos_2016 &lt;- dptos_2016 %&gt;% mutate(LATITUD = unlist(map(dptos_2016$geometry,1)), LONGITUD = unlist(map(dptos_2016$geometry,2))) %&gt;% st_set_geometry(NULL) 7.4.2 Codificar variables categóricas Rara vez es posible utilizar columnas categóricas en modelos estadísticos, pero por suerte podemos recurrir a la alternativa de reemplazar una columna de datos categóricos por una serie de variables binarias, o “dummy”. En nuestro dataset seleccionamos solamente una variable categórica: BARRIO. Entonces, en lugar de… dpto BARRIO A PALERMO B BELGRANO C SAN TELMO … deberíamos tener algo parecido a: caso PALERMO BELGRANO SAN TELMO A 1 0 0 B 0 1 0 C 0 0 1 Para evitar futuros problemas por tener espacios en los encabezados de las nuevas columnas, comencemos reemplazando los &quot; &quot; en los nombres de barrios por un &quot;_&quot;. Por ejemplo, en vez de decir SAN TELMO, que pase a decir SAN_TELMO. dptos_2016 &lt;- dptos_2016 %&gt;% mutate(BARRIO=str_replace_all(BARRIO, &quot; &quot;, &quot;_&quot;)) Como buen lenguaje creado por y para practicantes del análisis estadístico, R trae una función específica para realizar esta tarea: model.matrix() que se usa de la siguiente forma: matriz_categorias_barrios &lt;- model.matrix(data = dptos_2016, ~ BARRIO - 1) y el resultado es, ni más ni menos, una matriz de variables binarias que representan las categorías originales: head(matriz_categorias_barrios) ## BARRIOAGRONOMIA BARRIOALMAGRO BARRIOBALVANERA BARRIOBARRACAS BARRIOBELGRANO ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 0 0 0 ## 4 0 0 0 0 0 ## 5 0 0 0 0 0 ## 6 0 0 0 0 0 ## BARRIOBOCA BARRIOBOEDO BARRIOCABALLITO BARRIOCHACARITA BARRIOCOGHLAN ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 0 0 0 ## 4 0 0 0 0 0 ## 5 0 0 0 0 0 ## 6 0 0 0 0 0 ## BARRIOCOLEGIALES BARRIOCONSTITUCION BARRIOFLORES BARRIOFLORESTA BARRIOLINIERS ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 0 0 0 ## 4 0 0 0 0 0 ## 5 0 0 0 0 0 ## 6 0 0 0 0 0 ## BARRIOMATADEROS BARRIOMONSERRAT BARRIOMONTE_CASTRO BARRIONUEVA_POMPEYA ## 1 0 0 0 0 ## 2 0 0 0 0 ## 3 0 0 0 0 ## 4 0 0 0 0 ## 5 0 0 0 0 ## 6 0 0 0 0 ## BARRIONUÑEZ BARRIOPALERMO BARRIOPARQUE_AVELLANEDA BARRIOPARQUE_CHACABUCO ## 1 0 1 0 0 ## 2 0 1 0 0 ## 3 0 1 0 0 ## 4 0 1 0 0 ## 5 0 0 0 0 ## 6 0 0 0 0 ## BARRIOPARQUE_CHAS BARRIOPARQUE_PATRICIOS BARRIOPATERNAL BARRIOPUERTO_MADERO ## 1 0 0 0 0 ## 2 0 0 0 0 ## 3 0 0 0 0 ## 4 0 0 0 0 ## 5 0 0 0 0 ## 6 0 0 0 0 ## BARRIORECOLETA BARRIORETIRO BARRIOSAAVEDRA BARRIOSAN_CRISTOBAL ## 1 0 0 0 0 ## 2 0 0 0 0 ## 3 0 0 0 0 ## 4 0 0 0 0 ## 5 0 0 0 0 ## 6 0 0 0 0 ## BARRIOSAN_NICOLAS BARRIOSAN_TELMO BARRIOVELEZ_SARSFIELD BARRIOVERSALLES ## 1 0 0 0 0 ## 2 0 0 0 0 ## 3 0 0 0 0 ## 4 0 0 0 0 ## 5 0 0 0 0 ## 6 0 0 0 0 ## BARRIOVILLA_CRESPO BARRIOVILLA_DEL_PARQUE BARRIOVILLA_DEVOTO ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 0 0 ## 5 1 0 0 ## 6 1 0 0 ## BARRIOVILLA_GRAL._MITRE BARRIOVILLA_LUGANO BARRIOVILLA_LURO ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 0 0 ## 5 0 0 0 ## 6 0 0 0 ## BARRIOVILLA_ORTUZAR BARRIOVILLA_PUEYRREDON BARRIOVILLA_REAL ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 0 0 ## 5 0 0 0 ## 6 0 0 0 ## BARRIOVILLA_SANTA_RITA BARRIOVILLA_SOLDATI BARRIOVILLA_URQUIZA ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 0 0 ## 5 0 0 0 ## 6 0 0 0 En breves agregaremos la matriz a nuestro dataframe de departamentos, pero antes terminemos con algunos ajustes que nos quedaron pendientes. 7.4.3 Unificar la escala de las variables numéricas Este paso siempre es necesario cuando estamos trabajando con variables que utilizan distintas unidades de medida. Aquí tenemos superficies, ambientes, años de antigüedad… de todo. Muchos algoritmos asumen que todas las variables tienen escalas comparables, lo cual genera problemas con las que alcanzan los valores más altos (como m2, que llega a 730) versus las que tienen rangos mucho menores (como cantidad de baños, que llega a 6). Si las dejásemos así, varias de las técnicas habituales del ML adjudicarían mucho más peso a las variables con números grandes, “despreciando” a las que por su naturaleza se mueven en rango más reducidos. En todo caso, no importa lo disímiles que sean las unidades de medida, la solución es simple: convertimos todas las variables a la famosa “distribución Z”, o función de estandarización, que convierte variables a una escala sin unidad de medida, que expresa cada valor como la cantidad de desvíos estándar que lo alejan de la media. Expresar todas las variables numéricas en forma de “z scores”, o “valores z”, las hace directamente comparables entre sí. En R disponemos de la función scale(), que obtiene los z-scores. Tomaremos entonces nuestro dataframe y usaremos mutate_all() para aplicar una función a todas las columnas restantes de un tirón. Eso si, quitando antes ciertas variables: las variables categóricas (que no tiene sentido pasar a z-scores porque no son variables numéricas), y la variable que estamos intentando predecir, ya que su escala no afecta los modelos y podemos dejarla en su formato original fácil de interpretar. dptos_2016 &lt;- dptos_2016 %&gt;% select(-BARRIO) %&gt;% mutate_all(funs(scale)) %&gt;% mutate(U_S_M2 = dptos_2016$U_S_M2) summary(dptos_2016) ## M2.V1 M2CUB.V1 U_S_M2 AMBIENTES.V1 ## Min. :-1.139657 Min. :-1.128859 Min. : 608 Min. :-1.648294 ## 1st Qu.:-0.585166 1st Qu.:-0.555401 1st Qu.: 2230 1st Qu.:-0.562206 ## Median :-0.329247 Median :-0.331005 Median : 2631 Median :-0.562206 ## Mean : 0.000000 Mean : 0.000000 Mean : 2768 Mean : 0.000000 ## 3rd Qu.: 0.203918 3rd Qu.: 0.217519 3rd Qu.: 3107 3rd Qu.: 0.523882 ## Max. :14.108855 Max. :14.080225 Max. :10555 Max. : 8.126499 ## ANTIGUEDAD.V1 BAÑOS.V1 M2DESC.V1 ## Min. :-0.538418 Min. :-0.541044 Min. :-0.520562 ## 1st Qu.:-0.538418 1st Qu.:-0.541044 1st Qu.:-0.520562 ## Median :-0.538418 Median :-0.541044 Median :-0.265463 ## Mean : 0.000000 Mean : 0.000000 Mean : 0.000000 ## 3rd Qu.: 0.029315 3rd Qu.: 1.093465 3rd Qu.:-0.010364 ## Max. : 6.337456 Max. : 7.631505 Max. :17.463925 ## DIST_SUBTE.V1 LATITUD.V1 LONGITUD.V1 ## Min. :-0.869389 Min. :-2.8800908 Min. :-3.404775 ## 1st Qu.:-0.522573 1st Qu.:-0.6580219 1st Qu.:-0.892539 ## Median :-0.282983 Median :-0.0170810 Median :-0.085086 ## Mean : 0.000000 Mean : 0.0000000 Mean : 0.000000 ## 3rd Qu.: 0.068419 3rd Qu.: 0.7382813 3rd Qu.: 0.706665 ## Max. : 6.102341 Max. : 2.3325377 Max. : 2.520604 Obsérvese que scale() mediante, ahora todas las variables (menos U_S_M2) tienen promedio igual a 0, y se mueven en el mismo rango sin que esto haya cambiado la forma de las distribuciones. Comparemos los “nuevos” histogramas con los que examinamos al inicio: La superficie total: ggplot() + geom_histogram(data = dptos_2016, aes(x = M2)) La superficie descubierta: ggplot() + geom_histogram(data = dptos_2016, aes(x = M2DESC)) La antigüedad de las viviendas: ggplot() + geom_histogram(data = dptos_2016, aes(x = ANTIGUEDAD)) ¡Las formas son iguales! no hemos hemos perdido “información” respecto a que tan típico o extremo es cada valor, y hemos ganado la posibilidad de comparar en forma directa todas las variables. 7.4.4 Consolidar todas las variables generadas ad-hoc en un sólo dataframe Nos ha quedado por un lado un dataframe de variables numéricas estandarizadas, y por otro una matriz que representa la pertenencia de cada departamento a un barrio de la Ciudad. Primero convertimos la matriz de barrios en dataframe (paso simple ya que estas estructuras de datos son muy similares entre si), y luego unimos las columnas de ambos con la función cbind(): matriz_categorias_barrios &lt;- as.data.frame(matriz_categorias_barrios) dptos_2016 &lt;- dptos_2016 %&gt;% cbind(matriz_categorias_barrios) Ahora que ya tenemos tenemos los datos limpios y en orden, estamos en condiciones de comenzar con nuestro modelo predictivo. 7.5 Paso 4: Crear sets de entrenamiento y de testeo Para poder evaluar la calidad de un modelo predictivo, es práctica común dividir los datos disponibles en dos porciones. Una parte será utilizada para “entrenar” el modelo de ML, es decir se le permitirá al algoritmo acceder a esos datos para establecer/aprender la forma en que cada variable predictora incide en la que se quiere predecir. El resto será preservado y utilizado para “tomarle examen” al modelo: se le mostraran sólo las variables predictoras de esos datos y se le pedirá una predicción del valor para cada una. Por último, contrastando aciertos y errores, se podrá establecer el grado de precisión del modelo. Incluso podríamos tener varios modelos distintos, obtenidos con distintas técnicas de ML. No es difícil, ya que una vez que los datos han sido obtenidos y preparados, nada impide usarlos como insumo de distintos algoritmos. En ese caso, se puede comparar la performance de los distintos modelos evaluando cual acierta mejor con la data de testeo. Definamos entonces cuales son las filas que van a incluirse en el set de entrenamiento, y cuáles en el de testeo, eligiéndolas al azar. De acuerdo a distintas recetas, a veces se separa el 90% de los datos para entrenamiento y el resto para testeo, otras veces es mitad y mitad… ya que siempre es más o menos arbitrario, aquí usaremos el 80% para entrenar, y el 20% para testear. #definimos a mano la &quot;semilla&quot; de aleatorización para obtener resultados reproducibles set.seed(1111) Tomamos al azar el 80% de las posiciones entre 1 y la cantidad total de filas de nuestro dataset: seleccion &lt;- sample(1:nrow(dptos_2016), size = nrow(dptos_2016) * 0.8) entrenamiento &lt;- dptos_2016 %&gt;% filter(row_number() %in% seleccion) # el testeo es el set opuesto - aquellas filas cuya posición no está entre las seleccionadas # el operador ! convierte una proposición en negativa testeo &lt;- dptos_2016 %&gt;% filter(!(row_number() %in% seleccion)) Veamos cuantas observaciones quedaron en cada set de datos: dim(entrenamiento) ## [1] 4942 57 dim(testeo) ## [1] 1236 57 Ahora si, por fin, apliquemos un poco de machine learning. 7.5.1 Paso 5: Entrenar y testear un modelo Random Forest, una implementación de árboles de decisión como los ilustrados en “Una introducción visual al machine learning”: modelo_RF &lt;- randomForest(data = entrenamiento, U_S_M2 ~ ., ntree = 500, importance = TRUE) # el parámetro &quot;importance&quot;: Define si el modelo estimará la importancia relativa de cada predictor en la calidad de la predicción -es decir, cuales variables son más importantes para predecir # resultados: modelo_RF ## ## Call: ## randomForest(formula = U_S_M2 ~ ., data = entrenamiento, ntree = 500, importance = TRUE) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 18 ## ## Mean of squared residuals: 166450 ## % Var explained: 76.48 Según lo que dice aquí, el modelo puede explicar casi el 80% de la varianza de valores encontrada entre los departamentos en venta en 2016 en base a todas las variables predictoras que decidimos emplear. ¿Qué tiene dentro el modelo? summary(modelo_RF) ## Length Class Mode ## call 5 -none- call ## type 1 -none- character ## predicted 4942 -none- numeric ## mse 500 -none- numeric ## rsq 500 -none- numeric ## oob.times 4942 -none- numeric ## importance 112 -none- numeric ## importanceSD 56 -none- numeric ## localImportance 0 -none- NULL ## proximity 0 -none- NULL ## ntree 1 -none- numeric ## mtry 1 -none- numeric ## forest 11 -none- list ## coefs 0 -none- NULL ## y 4942 -none- numeric ## test 0 -none- NULL ## inbag 0 -none- NULL ## terms 3 terms call De todo! Por ejemplo, “type” nos emite confirmar qué tipo de análisis realizó: Fue de regresión en este caso, pero podría haber sido otro, como clasificación (cuando se predice un atributo categórico en lugar de una variable continua): modelo_RF$type ## [1] &quot;regression&quot; O “importance”, que contiene un ranking con la importancia relativa de cada predictor, es decir cuáles son los que más ayudan a estimar el valor a predecir (U_S_M2): modelo_RF$importance ## %IncMSE IncNodePurity ## M2 170405.3782376 270645569.04 ## M2CUB 134294.3277435 221452113.17 ## AMBIENTES 48984.5707206 63495159.15 ## ANTIGUEDAD 182327.5412411 245711058.04 ## BAÑOS 74607.5970445 112362124.22 ## M2DESC 180674.2790710 261275881.96 ## DIST_SUBTE 176521.6801680 264617638.20 ## LATITUD 413992.5453703 500524607.66 ## LONGITUD 699599.1593140 732943742.64 ## BARRIOAGRONOMIA 6.1109342 50231.45 ## BARRIOALMAGRO 3425.0417850 5174568.21 ## BARRIOBALVANERA 5165.7775106 12632858.63 ## BARRIOBARRACAS 602.4320171 1359571.12 ## BARRIOBELGRANO 53703.7830256 82005552.59 ## BARRIOBOCA 2226.6645687 8059977.35 ## BARRIOBOEDO 129.7679756 692676.22 ## BARRIOCABALLITO 24001.5913160 22779193.75 ## BARRIOCHACARITA 849.1302415 5008125.78 ## BARRIOCOGHLAN 147.9159700 842049.34 ## BARRIOCOLEGIALES 1734.7723756 5905437.45 ## BARRIOCONSTITUCION 1406.6594452 3775022.48 ## BARRIOFLORES 623.3206156 2194421.66 ## BARRIOFLORESTA 50.9720032 548406.17 ## BARRIOLINIERS 208.3842665 444341.83 ## BARRIOMATADEROS 156.9745908 767955.35 ## BARRIOMONSERRAT 3457.9645045 6058827.59 ## BARRIOMONTE_CASTRO 285.6709069 771603.69 ## BARRIONUEVA_POMPEYA 0.6181519 102772.05 ## BARRIONUÑEZ 4040.6787236 6723864.03 ## BARRIOPALERMO 87558.1001621 135914531.21 ## BARRIOPARQUE_AVELLANEDA 0.4058692 47294.50 ## BARRIOPARQUE_CHACABUCO 23745.2613546 25354031.16 ## BARRIOPARQUE_CHAS 3413.2075882 12766924.31 ## BARRIOPARQUE_PATRICIOS 488.4688861 793202.79 ## BARRIOPATERNAL 0.0000000 126268.58 ## BARRIOPUERTO_MADERO 62005.3709601 188295054.36 ## BARRIORECOLETA 22937.5507754 26385418.00 ## BARRIORETIRO 1838.6391808 7408843.36 ## BARRIOSAAVEDRA 906.9669375 2343163.07 ## BARRIOSAN_CRISTOBAL 19076.5460187 15731802.46 ## BARRIOSAN_NICOLAS 2205.8557488 3070218.74 ## BARRIOSAN_TELMO 68.4319709 1541670.81 ## BARRIOVELEZ_SARSFIELD -5.7358774 200321.94 ## BARRIOVERSALLES -0.1234807 28998.64 ## BARRIOVILLA_CRESPO 20471.7150953 17075092.55 ## BARRIOVILLA_DEL_PARQUE 84.2250373 665843.63 ## BARRIOVILLA_DEVOTO -36.1429484 1687903.42 ## BARRIOVILLA_GRAL._MITRE 42.4190298 408725.72 ## BARRIOVILLA_LUGANO 221.7012795 2029868.93 ## BARRIOVILLA_LURO 7.6858899 621766.65 ## BARRIOVILLA_ORTUZAR 232.3501178 1753655.20 ## BARRIOVILLA_PUEYRREDON 185.8436268 750870.65 ## BARRIOVILLA_REAL 4.2441112 33564.97 ## BARRIOVILLA_SANTA_RITA 37.5218071 324228.64 ## BARRIOVILLA_SOLDATI 173.9582226 1532857.54 ## BARRIOVILLA_URQUIZA 892.7016258 2082640.74 La columna “%IncMSE” representa el porcentaje de error promedio, la magnitud en la que el valor predicho por el modelo difiere del valor observado, cuando cada predictor se retira del modelo (es decir, cuanto peor sería la predicción si no se usara). Por eso los números mayores están asociados a los predictores de más peso, que en este caso son LONGITUD, LATITUD, M2DESC, ANTIGUEDAD y DIST_SUBTE. Además de encontrar la correlación esperable entre el valor del m2 y la superficie descubierta de las propiedades, los años de antiguedad y la distancia al subte, nuestro modelo ha encontrado que la ubicación (latitud y longitud) es la clave del valor de la propiedad… y sin saber nada de geografía ni urbanismo. En “predicted” tenemos la mediana del valor del m2 predicho para cada observación: head(modelo_RF$predicted) ## 2 3 4 5 7 8 ## 2644.151 3306.486 3782.182 2252.104 2144.658 2977.580 Aprovechando que dentro del modelo, “y” contiene los valores observados, evaluemos en forma gráfica cuánto se aproximan las predicciones de cada departamento al valor real (el observado): ggplot() + geom_point(aes(x = modelo_RF$predicted, y = modelo_RF$y), alpha = 0.3) Se ajusta bastante bien. Pero ahora veremos una manera de cuantificar la precisión del modelo. 7.5.2 Midiendo la performance del modelo contra datos que no conoce Veamos ahora como se comporta nuestro modelo cuando debe predecir valores de observaciones que no se han utilizado para el entrenamiento, los que reservamos para el set de testeo. predicciones_test &lt;- predict(modelo_RF, newdata = testeo) head(predicciones_test) ## 1 6 12 18 20 21 ## 3524.172 2303.221 3805.219 3140.920 3200.704 3059.844 En un gráfico: ggplot() + geom_point(aes(x = predicciones_test, y = testeo$U_S_M2), alpha = 0.3) El gráfico es muy similar e incluso parecería que se ajusta mejor que con los datos ya conocidos utilizados para el entreneamiento. 7.5.3 Comparando performance Es práctico obtener un sólo número, un indicador simple que nos diga que tan bien predice el modelo, y así poder comparar distintos modelos entre si (o distintos datasets contra el mismo modelo) utilizando esa medida. En estadística es común el uso del RMSE como indicador de grado de ajuste, o “Root Mean Square Error” - la raíz cuadrada de la media de los errores al cuadrado. El modelo incluye el MSE (o sea la suma de los errores al cuadrado) que surge de comparar predicciones con valores observados. Y en el caso de un random forest, que intenta muchos árboles distintos, varios MSEs resultantes: 500 en nuestro caso, uno por cada árbol trazado. Tomamos la media de todos los MSE para obtener un valor general, y luego tomamos la raíz cuadrada para obtener el RMSE: RMSE &lt;- modelo_RF$mse %&gt;% mean() %&gt;% sqrt() RMSE ## [1] 414.0307 Eso significa que la diferencia promedio entre valor esperado y valor hallado para cada distrito fue de 414.0306552 dólares. Y en comparación, ¿Qué tan bueno resultó el modelo cuando se aplicó a datos que no conocía? RMSE_test &lt;- sqrt(mean((predicciones_test - testeo$U_S_M2)^2)) RMSE_test ## [1] 422.246 Con un valor medio de error de 422.2460377 dólares, el modelo ha funcionado muy bien con datos desconocidos, incluso mejorando levemente su performance respecto al set de training. Esto indica que no sufre de “overfitting”, la condición de estar excesivamente ajustado a los datos con los que fue entrenado. Por eso el modelo no pierde precisión cuando lidia con datos nuevos. Como despedida, hagamos un exámen visual y representamos en un gráfico cada valor predicho y cada valor observado para los datos de entrenamiento: ggplot() + geom_point(aes(x = 1:length(predicciones_test), y = predicciones_test), color = &quot;salmon&quot;, alpha = .5, size = .75) + geom_point(aes(x = 1:nrow(testeo), y = testeo$U_S_M2), color = &quot;lightblue3&quot;, alpha = .5, size = .75) + labs(x = &quot;valores predichos&quot;, y = &quot;valores observados&quot;) + theme_minimal() 7.6 Ejercicios Examinando y prediciendo dinámicas urbanas I. Elegir algun dataset relacionado a temas urbanos que contenga una variable que les resulte de interés como variable a predecir. Por ejemplo: valor del m2, población, etc. Realizar un modelo de árboles de decisión (Random Forest) a partir de los siguientes pasos: Realizar el análisis de variables y la limpieza necesaria. Crear set de entrenamiento y de testeo. Entrenar y testear el modelo: Medir la performance. "]
]
